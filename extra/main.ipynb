{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.4260\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "# Define translations and references\n",
    "translations = [\n",
    "    \"The bride's father symbolically offers to the bridegroom a cow as a present.\",\n",
    "    'Early pregnancy bleeding is usually from a maternal source, rather than a fetal one.'\n",
    "]\n",
    "\n",
    "references = [\n",
    "    '<2hi> ‡§¶‡•Å‡§≤‡•ç‡§π‡§® ‡§ï‡§æ ‡§™‡§ø‡§§‡§æ ‡§™‡•ç‡§∞‡§§‡•Ä‡§ï‡§æ‡§§‡•ç‡§Æ‡§ï ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§¶‡•Ç‡§≤‡•ç‡§π‡•á ‡§ï‡•ã ‡§è‡§ï ‡§ó‡§æ‡§Ø ‡§â‡§™‡§π‡§æ‡§∞ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§ </s>',\n",
    "    '<2hi> ‡§™‡•ç‡§∞‡§æ‡§∞‡§Ç‡§≠‡§ø‡§ï ‡§ó‡§∞‡•ç‡§≠‡§æ‡§µ‡§∏‡•ç‡§•‡§æ ‡§∞‡§ï‡•ç‡§§‡§∏‡•ç‡§∞‡§æ‡§µ ‡§Ü‡§Æ‡§§‡•å‡§∞ ‡§™‡§∞ ‡§≠‡•ç‡§∞‡•Ç‡§£ ‡§ï‡•á ‡§¨‡§ú‡§æ‡§Ø ‡§Æ‡§æ‡§§‡•É ‡§∏‡•ç‡§∞‡•ã‡§§ ‡§∏‡•á ‡§π‡•ã‡§§‡§æ ‡§π‡•à‡•§ </s>'\n",
    "]\n",
    "\n",
    "# Compute BLEU score\n",
    "bleu = sacrebleu.corpus_bleu(translations, [references])\n",
    "print(f\"BLEU Score: {bleu.score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Input: tensor([[  466,  1981,    80, 25573, 64001, 64004]])\n",
      "Tokenized Output: tensor([[64006,   942,    43, 32720,  8384, 64001]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Define the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/IndicBART\", do_lower_case=False, use_fast=False, keep_accents=True)\n",
    "\n",
    "# Input and Output sequences\n",
    "x = \"I am a boy </s> <2en>\"\n",
    "y = \"<2hi> ‡§Æ‡•à‡§Ç ‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡§æ ‡§π‡•Ç‡§Å </s>\"\n",
    "\n",
    "# Tokenize inputs and outputs\n",
    "inp = tokenizer(x, add_special_tokens=False, return_tensors=\"pt\", padding=True).input_ids\n",
    "out = tokenizer(y, add_special_tokens=False, return_tensors=\"pt\", padding=True).input_ids\n",
    "\n",
    "print(\"Tokenized Input:\", inp)\n",
    "print(\"Tokenized Output:\", out)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from src.gpt2 import GPT2Config, GPT2LMHeadModel\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64014\n",
      "64000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (-1). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Model \n",
      "Decoder only model \n",
      "\n",
      " I am a boy‡§ø‡§©‡•à‡§™‡§ü‡§ø‡§Ø‡•á ‡§®‡•Ä‡§ü‡§ø‡§≤‡•ã‡§™‡§ü‡§ø‡§Ø‡•á‡§™‡§ü‡§ø‡§Ø‡•á Srinagar ‡§±‡•á‡§°‡§ø‡§Ø‡•ã ‡§ú‡§Ø‡§º‡§®‡•ç‡§§ ‡§π‡•Ç‡§µ‡•Å ‡§ï‡•ã‡§¥‡§ø‡§ï‡•ç‡§ï‡•ã‡§ü‡•ç ‡§ï‡§∞‡•Å‡§õ‡§ø ‡§∞‡•Ü‡§ï‡§æ‡§∞‡•ç‡§°‡•ç‡§∏‡§¶‡§æ ‡§ö‡•Å‡§ï‡•á1 key‡§Æ‡§æ‡§®‡§ø‡§™‡§ü‡§ø‡§Ø‡•álateüçø ‡§≤‡§°‡§º‡§ï‡•á ‡§±‡•á‡§°‡§ø‡§Ø‡•ã ‡§ó‡•Å‡§∏‡•ç‡§∏‡•á ‡§Æ‡•Ü‡§§‡•ç‡§§ ‡§õ‡•ã‡§ü‡§¨‡•á‡§≤‡§æ transparencylate ‡§∞‡§ï‡•ç‡§§‡§¶‡§æ‡§¨Èπø‡§ï‡§°‡§º‡§æ ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡§æ‡§≤‡•Å mid‡§ï‡§°‡§º‡§æ ‡§õ‡•ã‡§ü‡§¨‡•á‡§≤‡§æ ‡§¨‡§ü‡§®‡•ç‡•Å‡§µ‡§ø‡§®‡•ç‡§±‡•Ü ‡§ï‡§§‡•ç‡§§‡•ã‡§≤‡§ø‡§ï‡•ç‡§ï ‡§Ü‡§æ‡•ç‡§∏‡§¶‡§æ ‡§±‡•á‡§°‡§ø‡§Ø‡•ã‡§™‡§ü‡§ø‡§Ø‡•á ‡§±‡•á‡§°‡§ø‡§Ø‡•ã ‡§í‡§∞‡§æ‡§∏‡§∏‡•ç\n",
      "145339392\n",
      "147490318\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder-Only Model Trainable Parameters: 57038592\n",
      "Encoder-Decoder Model Trainable Parameters: 145339392\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Decoder-Only Model Configuration\n",
    "dec_only_config = GPT2Config(\n",
    "    vocab_size=64014,\n",
    "    bos_token_id=64000,\n",
    "    n_layer=1,\n",
    "    pad_token_id=0,\n",
    "    eos_token_id=64001\n",
    ")\n",
    "model = GPT2LMHeadModel(config=dec_only_config)\n",
    "\n",
    "print(\"Decoder-Only Model Trainable Parameters:\", model.num_parameters(only_trainable=True))\n",
    "print(\"Encoder-Decoder Model Trainable Parameters:\", enc_dec_model.num_parameters(only_trainable=True))\n",
    "\n",
    "# 57038592\n",
    "# 145339392"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "def compute_bleu_score(predictions, references):\n",
    "    return sacrebleu.corpus_bleu(predictions, [references]).score\n",
    "\n",
    "def compute_chrf_score(predictions, references):\n",
    "    score = sacrebleu.corpus_chrf(predictions, [references])\n",
    "    return score.score\n",
    "\n",
    "def compute_ter_score(predictions, references):\n",
    "    ter = sacrebleu.corpus_ter(predictions, [references])\n",
    "    return ter.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids=inp, labels=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(input_ids=inp, max_length=50, num_beams=5, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am a boy ‡§®‡§æ‡§ó‡§∞‡§ø‡§ï‡§æ‡§Ç‡§®‡§æ ‡§∞‡§ø‡§ú‡§≠‡•Ä ‡§∞‡§ø‡§ú‡§≠‡•Ä ‡§∞‡§ø‡§ú‡§≠‡•Ä„ÇΩ42424242 ‡§ï‡•Å‡§±‡§øüé°üé°üé° ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç suggested suggested suggested‡§æ‡§§‡§Æ‡§ï‡§æ‡§§‡§Æ‡§ï ‡§∏‡§Ç‡§¨‡§æ‡§¶‡§Æ‡§æ‡§ß‡•ç‡§Ø‡§Æ‡§æ‡§§‡§Æ‡§ï‡§æ‡§§‡§Æ‡§ï‡§æ‡§§‡§Æ‡§ï ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•çtestestes']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_score = compute_bleu_score(predictions, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.5799856917484751\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "def compute_bleu_score(predictions, references):\n",
    "    # Convert tensors to lists of strings\n",
    "    predictions_list = [str(pred) for pred in predictions]\n",
    "    references_list = [str(ref) for ref in references]\n",
    "\n",
    "    # Compute BLEU score\n",
    "    bleu = sacrebleu.corpus_bleu(predictions_list, [references_list])\n",
    "    return bleu.score\n",
    "\n",
    "bleu_score = compute_bleu_score(predictions, out)\n",
    "print(f\"BLEU Score: {bleu_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64014\n",
      "64000\n",
      "Encoder Model OWOWOWOWOWOWOWOWOWOWOWOWOWOWOWOWOWOW\n",
      "Decoder only model \n",
      "\n",
      " ['I am a boy ‡§®‡§æ‡§ó‡§∞‡§ø‡§ï‡§æ‡§Ç‡§®‡§æ ‡§∞‡§ø‡§ú‡§≠‡•Ä ‡§∞‡§ø‡§ú‡§≠‡•Ä ‡§∞‡§ø‡§ú‡§≠‡•Ä„ÇΩ42424242 ‡§ï‡•Å‡§±‡§øüé°üé°üé° ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç suggested suggested suggested‡§æ‡§§‡§Æ‡§ï‡§æ‡§§‡§Æ‡§ï ‡§∏‡§Ç‡§¨‡§æ‡§¶‡§Æ‡§æ‡§ß‡•ç‡§Ø‡§Æ‡§æ‡§§‡§Æ‡§ï‡§æ‡§§‡§Æ‡§ï‡§æ‡§§‡§Æ‡§ï ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•ç ‡§Æ‡§©‡§Æ‡•çtestestes']\n",
      "145339392\n",
      "147490318\n"
     ]
    }
   ],
   "source": [
    "from src.mbart.configuration_mbart import MBartConfig\n",
    "from src.mbart.modeling_mbart import MBartModel, MBartForConditionalGeneration\n",
    "from src.mbart.tokenization_mbart import MBartTokenizer\n",
    "\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "from transformers import AlbertTokenizer, AutoTokenizer\n",
    "\n",
    "\n",
    "from src.xlnet.modeling_xlnet import XLNetLMHeadModel\n",
    "from src.xlnet.configuration_xlnet import XLNetConfig\n",
    "\n",
    "\n",
    "enc_dec_config = MBartConfig(vocab_size = 64014, bos_token_id= 64000, \n",
    "                     activation_dropout=0.1, attention_dropout=0.1,encoder_layers=2,\n",
    "                     decoder_layers=3,pad_token_id=0,eos_token_id=64001)\n",
    "enc_dec_model = MBartForConditionalGeneration(config=enc_dec_config)\n",
    "\n",
    "print(len(tokenizer))\n",
    "print(tokenizer.vocab_size)\n",
    "\n",
    "dec_only_config = XLNetConfig(vocab_size = 64014, bos_token_id= 64000, n_layer=6,pad_token_id=0,eos_token_id=64001)\n",
    "dec_only_model = XLNetLMHeadModel(config=dec_only_config)\n",
    "\n",
    "x = \"I am a boy </s> <2en>\"\n",
    "y = \"<2hi> ‡§Æ‡•à‡§Ç  ‡§è‡§ï ‡§≤‡•ú‡§ï‡§æ ‡§π‡•Ç‡§Å </s>\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inp = tokenizer(x, add_special_tokens=False, return_tensors=\"pt\", padding=True).input_ids\n",
    "out = tokenizer(y, add_special_tokens=False, return_tensors=\"pt\", padding=True).input_ids \n",
    "\n",
    "\n",
    "output_enc_dec = enc_dec_model(input_ids=inp, decoder_input_ids=out[:,0:-1], labels=out[:,1:])\n",
    "\n",
    "output_dec = dec_only_model(input_ids=inp, labels=out)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bos_id = tokenizer._convert_token_to_id_with_added_voc(\"<s>\")\n",
    "eos_id = tokenizer._convert_token_to_id_with_added_voc(\"</s>\")\n",
    "pad_id = tokenizer._convert_token_to_id_with_added_voc(\"<pad>\")\n",
    "\n",
    "enc_dec_out = tokenizer.decode(\n",
    "    enc_dec_model.generate(inp, use_cache=True, \n",
    "                           num_beams=4, max_length=20, min_length=1, early_stopping=True, pad_token_id=pad_id, \n",
    "                           bos_token_id=bos_id, eos_token_id=eos_id,\n",
    "                           decoder_start_token_id=tokenizer._convert_token_to_id_with_added_voc(\"<2en>\"))[0]\n",
    ", skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "# enc_dec_out = tokenizer.decode(enc_dec_model.generate(inp, max_length=50, num_return_sequences=1)[0], skip_special_tokens=True)\n",
    "\n",
    "dec_only_out = dec_only_model.generate(inp, max_length=50, num_return_sequences=1)\n",
    "dec_only_out  = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outputs]\n",
    "print(\"Encoder Model\",enc_dec_out)\n",
    "print(\"Decoder only model \\n\\n\",dec_only_out)\n",
    "\n",
    "\n",
    "\n",
    "print(enc_dec_model.num_parameters(only_trainable=True))\n",
    "print(dec_only_model.num_parameters(only_trainable=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[64006,   942,    43, 32720,  8384, 64001]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.5799856917484751\n"
     ]
    }
   ],
   "source": [
    "bleu_score = compute_bleu_score(dec_only_out, out)\n",
    "print(f\"BLEU Score: {bleu_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
