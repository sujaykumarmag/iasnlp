{"cells":[{"cell_type":"markdown","metadata":{},"source":["**For singleData**"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T04:00:48.367165Z","iopub.status.busy":"2024-07-03T04:00:48.366815Z","iopub.status.idle":"2024-07-03T04:00:48.682681Z","shell.execute_reply":"2024-07-03T04:00:48.681587Z","shell.execute_reply.started":"2024-07-03T04:00:48.367137Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['translations'],\n","        num_rows: 32216\n","    })\n","    validation: Dataset({\n","        features: ['translations'],\n","        num_rows: 4027\n","    })\n","    test: Dataset({\n","        features: ['translations'],\n","        num_rows: 4027\n","    })\n","})\n"]}],"source":["import pandas as pd\n","from datasets import Dataset, DatasetDict\n","\n","file_eng_latn_mal_mlym = \"/kaggle/input/wiki-datatrans/wiki/eng_Latn-hin_Deva/train.eng_Latn\"\n","file_mal_mlym = \"/kaggle/input/wiki-datatrans/wiki/eng_Latn-hin_Deva/train.hin_Deva\"\n","\n","with open(file_eng_latn_mal_mlym, 'r', encoding='utf-8') as f:\n","    eng_data = f.readlines()\n","\n","with open(file_mal_mlym, 'r', encoding='utf-8') as f:\n","    mal_data = f.readlines()\n","\n","combined_data = [\n","    f\"{eng.strip()} #hi#> {mal.strip()}\"\n","    for eng, mal in zip(eng_data, mal_data)\n","]\n","\n","df = pd.DataFrame(combined_data, columns=[\"translations\"])\n","\n","train_df = df.sample(frac=0.8, random_state=42)\n","temp_df = df.drop(train_df.index)\n","validation_df = temp_df.sample(frac=0.5, random_state=42)\n","test_df = temp_df.drop(validation_df.index)\n","\n","# Step 4: Create Dataset objects and remove the '__index_level_0__' column\n","train_dataset = Dataset.from_pandas(train_df).remove_columns(['__index_level_0__'])\n","validation_dataset = Dataset.from_pandas(validation_df).remove_columns(['__index_level_0__'])\n","test_dataset = Dataset.from_pandas(test_df).remove_columns(['__index_level_0__'])\n","\n","# Step 5: Create a DatasetDict\n","dataset_dict = DatasetDict({\n","    'train': train_dataset,\n","    'validation': validation_dataset,\n","    'test': test_dataset\n","})\n","\n","print(dataset_dict)\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T04:00:39.053078Z","iopub.status.busy":"2024-07-03T04:00:39.052528Z","iopub.status.idle":"2024-07-03T04:00:39.064383Z","shell.execute_reply":"2024-07-03T04:00:39.063500Z","shell.execute_reply.started":"2024-07-03T04:00:39.053047Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['translations'],\n","        num_rows: 32215\n","    })\n","    validation: Dataset({\n","        features: ['translations'],\n","        num_rows: 1000\n","    })\n","    test: Dataset({\n","        features: ['translations'],\n","        num_rows: 1000\n","    })\n","})\n"]}],"source":["from datasets import DatasetDict\n","\n","new_train_dataset = dataset_dict['train'].select(range(32215))\n","new_validation_dataset = dataset_dict['validation'].select(range(1000))\n","new_test_dataset = dataset_dict['test'].select(range(1000))\n","\n","new_dataset_dict = DatasetDict({\n","    'train': new_train_dataset,\n","    'validation': new_validation_dataset,\n","    'test': new_test_dataset\n","})\n","print(new_dataset_dict)"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T08:40:39.611333Z","iopub.status.busy":"2024-07-03T08:40:39.610703Z","iopub.status.idle":"2024-07-03T08:40:39.624610Z","shell.execute_reply":"2024-07-03T08:40:39.623582Z","shell.execute_reply.started":"2024-07-03T08:40:39.611302Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'Click the \"Search\" button to begin the search. #hi#> à¤–à¥‹à¤œ à¤ªà¥à¤°à¤¾à¤°à¤‚à¤­ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ \"à¤–à¥‹à¤œà¥‡à¤‚\" à¤•à¤¾ à¤¬à¤Ÿà¤¨ à¤¦à¤¬à¤¾à¤à¤à¥¤'"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["new_dataset_dict['test']['translations'][9]"]},{"cell_type":"markdown","metadata":{},"source":["# SetUp Directory"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-06-29T08:45:27.190458Z","iopub.status.busy":"2024-06-29T08:45:27.190112Z","iopub.status.idle":"2024-06-29T08:45:27.205435Z","shell.execute_reply":"2024-06-29T08:45:27.204537Z","shell.execute_reply.started":"2024-06-29T08:45:27.190429Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'/kaggle/working'"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["%pwd"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-29T08:45:29.184387Z","iopub.status.busy":"2024-06-29T08:45:29.184043Z","iopub.status.idle":"2024-06-29T08:45:29.190500Z","shell.execute_reply":"2024-06-29T08:45:29.189552Z","shell.execute_reply.started":"2024-06-29T08:45:29.184358Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle\n"]}],"source":["%cd .."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-29T08:45:30.120553Z","iopub.status.busy":"2024-06-29T08:45:30.119916Z","iopub.status.idle":"2024-06-29T08:45:31.062110Z","shell.execute_reply":"2024-06-29T08:45:31.061131Z","shell.execute_reply.started":"2024-06-29T08:45:30.120518Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[0m\u001b[01;34minput\u001b[0m/  \u001b[01;34mlib\u001b[0m/  \u001b[01;34mworking\u001b[0m/\n"]}],"source":["%ls"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T04:01:06.136247Z","iopub.status.busy":"2024-07-03T04:01:06.135880Z","iopub.status.idle":"2024-07-03T04:01:07.102789Z","shell.execute_reply":"2024-07-03T04:01:07.101711Z","shell.execute_reply.started":"2024-07-03T04:01:06.136216Z"},"trusted":true},"outputs":[],"source":["# %mkdir working/results/\n","![ ! -d working/results/ ] && mkdir -p working/results/"]},{"cell_type":"markdown","metadata":{},"source":["# Necessary Installs and Imports"]},{"cell_type":"markdown","metadata":{},"source":["## Installs"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T04:01:10.292505Z","iopub.status.busy":"2024-07-03T04:01:10.291645Z","iopub.status.idle":"2024-07-03T04:01:53.708721Z","shell.execute_reply":"2024-07-03T04:01:53.707551Z","shell.execute_reply.started":"2024-07-03T04:01:10.292470Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.2)\n","Collecting datasets\n","  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\n","Collecting transformers\n","  Downloading transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting trl\n","  Downloading trl-0.9.4-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\n","Collecting accelerate\n","  Downloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\n","Collecting peft\n","  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n","Collecting bitsandbytes\n","  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\n","Collecting pyarrow>=15.0.0 (from datasets)\n","  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n","Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.1)\n","Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.3.1)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.23.2)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\n","Requirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl) (2.1.2)\n","Collecting tyro>=0.5.11 (from trl)\n","  Downloading tyro-0.8.5-py3-none-any.whl.metadata (8.2 kB)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (1.12.1)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.1.2)\n","Collecting docstring-parser>=0.16 (from tyro>=0.5.11->trl)\n","  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (13.7.0)\n","Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n","  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.17.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.3)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n","Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading transformers-4.42.3-py3-none-any.whl (9.3 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading trl-0.9.4-py3-none-any.whl (226 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading tyro-0.8.5-py3-none-any.whl (103 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n","Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n","Installing collected packages: shtab, pyarrow, docstring-parser, tyro, bitsandbytes, accelerate, transformers, datasets, trl, peft\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 14.0.2\n","    Uninstalling pyarrow-14.0.2:\n","      Successfully uninstalled pyarrow-14.0.2\n","  Attempting uninstall: docstring-parser\n","    Found existing installation: docstring-parser 0.15\n","    Uninstalling docstring-parser-0.15:\n","      Successfully uninstalled docstring-parser-0.15\n","  Attempting uninstall: accelerate\n","    Found existing installation: accelerate 0.30.1\n","    Uninstalling accelerate-0.30.1:\n","      Successfully uninstalled accelerate-0.30.1\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.41.2\n","    Uninstalling transformers-4.41.2:\n","      Successfully uninstalled transformers-4.41.2\n","  Attempting uninstall: datasets\n","    Found existing installation: datasets 2.19.2\n","    Uninstalling datasets-2.19.2:\n","      Successfully uninstalled datasets-2.19.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf 24.4.1 requires cubinlinker, which is not installed.\n","cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\n","cudf 24.4.1 requires ptxcompiler, which is not installed.\n","cuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","dask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\n","apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\n","apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\n","apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\n","beatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.1 which is incompatible.\n","cudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\n","cudf 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n","kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\n","rapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\n","rapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed accelerate-0.31.0 bitsandbytes-0.43.1 datasets-2.20.0 docstring-parser-0.16 peft-0.11.1 pyarrow-16.1.0 shtab-1.7.1 transformers-4.42.3 trl-0.9.4 tyro-0.8.5\n"]}],"source":["!pip install -U datasets transformers trl accelerate peft bitsandbytes"]},{"cell_type":"markdown","metadata":{},"source":["## HuggingFace SetUp"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from huggingface_hub import notebook_login\n","# notebook_login()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T01:39:41.100571Z","iopub.status.busy":"2024-03-14T01:39:41.099815Z","iopub.status.idle":"2024-03-14T01:39:42.663452Z","shell.execute_reply":"2024-03-14T01:39:42.662351Z","shell.execute_reply.started":"2024-03-14T01:39:41.100539Z"},"trusted":true},"outputs":[],"source":["!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\""]},{"cell_type":"markdown","metadata":{},"source":["## Imports"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T04:02:38.168890Z","iopub.status.busy":"2024-07-03T04:02:38.168010Z","iopub.status.idle":"2024-07-03T04:02:55.115781Z","shell.execute_reply":"2024-07-03T04:02:55.115004Z","shell.execute_reply.started":"2024-07-03T04:02:38.168851Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-07-03 04:02:45.932726: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-03 04:02:45.932830: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-03 04:02:46.074891: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n","from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel\n","from datasets import load_dataset\n","from trl import SFTTrainer\n","import torch"]},{"cell_type":"markdown","metadata":{},"source":["# Model SetUp"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T04:03:01.665627Z","iopub.status.busy":"2024-07-03T04:03:01.664559Z","iopub.status.idle":"2024-07-03T04:03:01.884022Z","shell.execute_reply":"2024-07-03T04:03:01.883062Z","shell.execute_reply.started":"2024-07-03T04:03:01.665597Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["from huggingface_hub import login\n","\n","# Log in to Hugging Face Hub\n","api_token = 'Your token'\n","login(api_token)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T04:03:03.818897Z","iopub.status.busy":"2024-07-03T04:03:03.818189Z","iopub.status.idle":"2024-07-03T04:14:14.658743Z","shell.execute_reply":"2024-07-03T04:14:14.657957Z","shell.execute_reply.started":"2024-07-03T04:03:03.818859Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"458e302005ef4892b3c5b3265436d73d","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"24abbc26d79b46bd90540a279744ffb3","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"01c506f2b70f46daa101eb7298bd0629","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e7c6878ea67c45e1bb3d09b3f3087d38","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3555abf82baa43e8a2733cb7aa96b3dc","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2f09e468cf9b4e53b969ec00c8b0125c","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"76b4204bc3234cefa8f8794bb4ed7d7f","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model_name = \"meta-llama/Llama-2-7b-hf\"\n","\n","compute_dtype = getattr(torch, \"float16\")\n","bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=compute_dtype,\n","            bnb_4bit_use_double_quant=True,\n",")\n","model = AutoModelForCausalLM.from_pretrained(model_name,quantization_config=bnb_config, device_map={\"\": 0})\n","model = prepare_model_for_kbit_training(model)"]},{"cell_type":"markdown","metadata":{},"source":["## Tokenizer"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T04:15:56.487064Z","iopub.status.busy":"2024-07-03T04:15:56.486434Z","iopub.status.idle":"2024-07-03T04:16:00.661906Z","shell.execute_reply":"2024-07-03T04:16:00.660912Z","shell.execute_reply.started":"2024-07-03T04:15:56.487031Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"59cb08b1d79549bab51ad02e7d47d51d","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2dd8a8136206416fa39de72eb9f8bcee","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0ee0a5f42d18402c8f9e61bca0a79173","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"450ce2fe217b426ca560e21b2c150ca5","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, add_eos_token=True)\n","tokenizer.pad_token = tokenizer.unk_token\n","tokenizer.padding_side = \"left\"\n","# tokenizer.add_special_tokens({'pad_token': '[PAD]'})"]},{"cell_type":"markdown","metadata":{},"source":["## Load Model"]},{"cell_type":"markdown","metadata":{},"source":["# LoRA Configuration"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T04:16:07.120617Z","iopub.status.busy":"2024-07-03T04:16:07.120241Z","iopub.status.idle":"2024-07-03T04:16:07.125459Z","shell.execute_reply":"2024-07-03T04:16:07.124579Z","shell.execute_reply.started":"2024-07-03T04:16:07.120588Z"},"trusted":true},"outputs":[],"source":["peft_config = LoraConfig(\n","            lora_alpha=16, \n","            lora_dropout=0.05,\n","            r=16,\n","            bias=\"none\",\n","            task_type=\"CAUSAL_LM\",\n","            target_modules= [\"down_proj\",\"up_proj\",\"gate_proj\"]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# peft_config = LoraConfig(\n","#             lora_alpha=16,\n","#             lora_dropout=0.05,\n","#             r=64,\n","#             bias=\"none\",\n","#             task_type=\"CAUSAL_LM\",\n","#             target_modules= [\"q_proj\",\"up_proj\",\"o_proj\",\"k_proj\",\"down_proj\",\"gate_proj\",\"v_proj\"]\n","# )"]},{"cell_type":"markdown","metadata":{},"source":["# Training Hyperparameters"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T04:16:12.852128Z","iopub.status.busy":"2024-07-03T04:16:12.851435Z","iopub.status.idle":"2024-07-03T04:16:12.883701Z","shell.execute_reply":"2024-07-03T04:16:12.882769Z","shell.execute_reply.started":"2024-07-03T04:16:12.852094Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]}],"source":["training_arguments = TrainingArguments(\n","        output_dir=\"working/results/\",\n","        evaluation_strategy=\"steps\",\n","        optim=\"paged_adamw_8bit\",\n","        save_steps=100,\n","        log_level=\"debug\",\n","        logging_steps=100,\n","        learning_rate=1e-4,\n","        eval_steps=100,\n","        fp16=True,\n","        do_eval=True,\n","        per_device_train_batch_size=48,\n","        per_device_eval_batch_size=48,\n","        gradient_accumulation_steps=2,\n","        warmup_steps=50,\n","        max_steps=500,\n","        lr_scheduler_type=\"linear\"\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Training with TRL"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T01:45:45.857270Z","iopub.status.busy":"2024-03-14T01:45:45.856339Z","iopub.status.idle":"2024-03-14T01:45:46.896707Z","shell.execute_reply":"2024-03-14T01:45:46.895448Z","shell.execute_reply.started":"2024-03-14T01:45:45.857229Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Thu Mar 14 01:45:46 2024       \n","\n","+---------------------------------------------------------------------------------------+\n","\n","| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n","\n","|-----------------------------------------+----------------------+----------------------+\n","\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","\n","|                                         |                      |               MIG M. |\n","\n","|=========================================+======================+======================|\n","\n","|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n","\n","| N/A   36C    P0              32W / 250W |   5362MiB / 16384MiB |      0%      Default |\n","\n","|                                         |                      |                  N/A |\n","\n","+-----------------------------------------+----------------------+----------------------+\n","\n","                                                                                         \n","\n","+---------------------------------------------------------------------------------------+\n","\n","| Processes:                                                                            |\n","\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","\n","|        ID   ID                                                             Usage      |\n","\n","|=======================================================================================|\n","\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T04:16:46.489088Z","iopub.status.busy":"2024-07-03T04:16:46.488378Z","iopub.status.idle":"2024-07-03T08:32:22.054331Z","shell.execute_reply":"2024-07-03T08:32:22.053337Z","shell.execute_reply.started":"2024-07-03T04:16:46.489054Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1961: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1961: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e29d1a2766ac454daca834eb09db47ff","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/32216 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e26f2a26bbfb4b4ca0333129a4276e83","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/4027 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:397: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n","  warnings.warn(\n","max_steps is given, it will override any value given in num_train_epochs\n","Using auto half precision backend\n","Currently training with a batch size of: 48\n","***** Running training *****\n","  Num examples = 32,216\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 48\n","  Total train batch size (w. parallel, distributed & accumulation) = 96\n","  Gradient Accumulation steps = 2\n","  Total optimization steps = 500\n","  Number of trainable parameters = 23,199,744\n","Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["wandb version 0.17.3 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.17.0"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240703_041721-9ytw7i5u</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/pmabhinav20/huggingface/runs/9ytw7i5u' target=\"_blank\">working/results/</a></strong> to <a href='https://wandb.ai/pmabhinav20/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/pmabhinav20/huggingface' target=\"_blank\">https://wandb.ai/pmabhinav20/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/pmabhinav20/huggingface/runs/9ytw7i5u' target=\"_blank\">https://wandb.ai/pmabhinav20/huggingface/runs/9ytw7i5u</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [500/500 4:14:14, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>2.213000</td>\n","      <td>1.670183</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>1.645900</td>\n","      <td>1.591565</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>1.603200</td>\n","      <td>1.566103</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>1.551400</td>\n","      <td>1.545185</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>1.529300</td>\n","      <td>1.538938</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","***** Running Evaluation *****\n","  Num examples = 4027\n","  Batch size = 48\n","We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n","Saving model checkpoint to working/results/checkpoint-100\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json\n","Model config LlamaConfig {\n","  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 11008,\n","  \"max_position_embeddings\": 4096,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 10000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float16\",\n","  \"transformers_version\": \"4.42.3\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","tokenizer config file saved in working/results/checkpoint-100/tokenizer_config.json\n","Special tokens file saved in working/results/checkpoint-100/special_tokens_map.json\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","\n","***** Running Evaluation *****\n","  Num examples = 4027\n","  Batch size = 48\n","Saving model checkpoint to working/results/checkpoint-200\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json\n","Model config LlamaConfig {\n","  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 11008,\n","  \"max_position_embeddings\": 4096,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 10000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float16\",\n","  \"transformers_version\": \"4.42.3\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","tokenizer config file saved in working/results/checkpoint-200/tokenizer_config.json\n","Special tokens file saved in working/results/checkpoint-200/special_tokens_map.json\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","\n","***** Running Evaluation *****\n","  Num examples = 4027\n","  Batch size = 48\n","Saving model checkpoint to working/results/checkpoint-300\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json\n","Model config LlamaConfig {\n","  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 11008,\n","  \"max_position_embeddings\": 4096,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 10000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float16\",\n","  \"transformers_version\": \"4.42.3\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","tokenizer config file saved in working/results/checkpoint-300/tokenizer_config.json\n","Special tokens file saved in working/results/checkpoint-300/special_tokens_map.json\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","\n","***** Running Evaluation *****\n","  Num examples = 4027\n","  Batch size = 48\n","Saving model checkpoint to working/results/checkpoint-400\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json\n","Model config LlamaConfig {\n","  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 11008,\n","  \"max_position_embeddings\": 4096,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 10000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float16\",\n","  \"transformers_version\": \"4.42.3\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","tokenizer config file saved in working/results/checkpoint-400/tokenizer_config.json\n","Special tokens file saved in working/results/checkpoint-400/special_tokens_map.json\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","\n","***** Running Evaluation *****\n","  Num examples = 4027\n","  Batch size = 48\n","Saving model checkpoint to working/results/checkpoint-500\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json\n","Model config LlamaConfig {\n","  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 11008,\n","  \"max_position_embeddings\": 4096,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 10000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float16\",\n","  \"transformers_version\": \"4.42.3\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","tokenizer config file saved in working/results/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in working/results/checkpoint-500/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"data":{"text/plain":["TrainOutput(global_step=500, training_loss=1.7085668029785157, metrics={'train_runtime': 15329.7478, 'train_samples_per_second': 3.131, 'train_steps_per_second': 0.033, 'total_flos': 9.15842475491328e+16, 'train_loss': 1.7085668029785157, 'epoch': 1.4880952380952381})"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["trainer = SFTTrainer(\n","        model=model,\n","        train_dataset=dataset_dict['train'],\n","        eval_dataset=dataset_dict['validation'],\n","        peft_config=peft_config,\n","        dataset_text_field=\"translations\",\n","        max_seq_length=48,\n","        tokenizer=tokenizer,\n","        args=training_arguments\n",")\n","\n","trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["# Inference: Translate with Llama 2"]},{"cell_type":"markdown","metadata":{},"source":["## Base Model SetUp"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T08:32:43.989028Z","iopub.status.busy":"2024-07-03T08:32:43.988650Z","iopub.status.idle":"2024-07-03T08:33:04.570240Z","shell.execute_reply":"2024-07-03T08:33:04.569300Z","shell.execute_reply.started":"2024-07-03T08:32:43.988997Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json\n","Model config LlamaConfig {\n","  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 11008,\n","  \"max_position_embeddings\": 4096,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 10000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float16\",\n","  \"transformers_version\": \"4.42.3\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n","loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/model.safetensors.index.json\n","Instantiating LlamaForCausalLM model under default dtype torch.float16.\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2\n","}\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5e9969af91da43ad83bc114f0cda9f9b","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["All model checkpoint weights were used when initializing LlamaForCausalLM.\n","\n","All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-hf.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n","loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/generation_config.json\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 1,\n","  \"do_sample\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 4096,\n","  \"pad_token_id\": 0,\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.model\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer_config.json\n"]}],"source":["base_model = \"meta-llama/Llama-2-7b-hf\"\n","compute_dtype = getattr(torch, \"float16\")\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=True,\n",")\n","model = AutoModelForCausalLM.from_pretrained(\n","        base_model, device_map={\"\": 0}, quantization_config=bnb_config\n",")\n","tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Initialize Adapter (Fine-Tuned-Model)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T08:33:10.488621Z","iopub.status.busy":"2024-07-03T08:33:10.488231Z","iopub.status.idle":"2024-07-03T08:33:11.035499Z","shell.execute_reply":"2024-07-03T08:33:11.034494Z","shell.execute_reply.started":"2024-07-03T08:33:10.488590Z"},"trusted":true},"outputs":[],"source":["# Fetched from Kaggle Output\n","model = PeftModel.from_pretrained(model, \"working/results/checkpoint-500/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Uploaded to Hugging Face Model Hub\n","# model = PeftModel.from_pretrained(model, \"musfiqdehan/Llama-2-7b-ft-mt-Bengali-to-English-sm\")"]},{"cell_type":"markdown","metadata":{},"source":["# Testing Manually"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T08:34:35.496884Z","iopub.status.busy":"2024-07-03T08:34:35.496508Z","iopub.status.idle":"2024-07-03T08:34:42.713814Z","shell.execute_reply":"2024-07-03T08:34:42.712811Z","shell.execute_reply.started":"2024-07-03T08:34:35.496854Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["à¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤†à¤ªà¤•à¤¾ à¤¨à¤¾à¤® à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ?\n"]}],"source":["my_text = \"Hello, what is your name?\"\n","\n","prompt = my_text+\" #hi#>\"\n","\n","tokenized_input = tokenizer(prompt, return_tensors=\"pt\")\n","input_ids = tokenized_input[\"input_ids\"].cuda()\n","\n","generation_output = model.generate(\n","        input_ids=input_ids,\n","        num_beams=6,\n","        return_dict_in_generate=True,\n","        output_scores=True,\n","        max_new_tokens=130\n",")\n","for seq in generation_output.sequences:\n","    output = tokenizer.decode(seq, skip_special_tokens=True)\n","    print(output.split(\"#hi#>\")[1].strip()) "]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T08:35:15.733256Z","iopub.status.busy":"2024-07-03T08:35:15.732860Z","iopub.status.idle":"2024-07-03T08:35:25.365740Z","shell.execute_reply":"2024-07-03T08:35:25.364880Z","shell.execute_reply.started":"2024-07-03T08:35:15.733223Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["à¤†à¤œ à¤®à¥‡à¤°à¥€ à¤œà¤¨à¥à¤®à¤¦à¤¿à¤¨ à¤¹à¥ˆ\n"]}],"source":["my_text = \"Today is my birthday\"\n","\n","prompt = my_text+\" #hi#>\"\n","\n","tokenized_input = tokenizer(prompt, return_tensors=\"pt\")\n","input_ids = tokenized_input[\"input_ids\"].cuda()\n","\n","generation_output = model.generate(\n","        input_ids=input_ids,\n","        num_beams=6,\n","        return_dict_in_generate=True,\n","        output_scores=True,\n","        max_new_tokens=130\n",")\n","for seq in generation_output.sequences:\n","    output = tokenizer.decode(seq, skip_special_tokens=True)\n","    print(output.split(\"#hi#>\")[1].strip()) "]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T08:38:05.113639Z","iopub.status.busy":"2024-07-03T08:38:05.112749Z","iopub.status.idle":"2024-07-03T08:38:12.860934Z","shell.execute_reply":"2024-07-03T08:38:12.858473Z","shell.execute_reply.started":"2024-07-03T08:38:05.113604Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["à¤–à¥‹à¤œ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ \"à¤–à¥‹à¤œ\" à¤¬à¤Ÿà¤¨ à¤•\n"]}],"source":["my_text = 'Click the \"Search\" button to begin the search'\n","\n","prompt = my_text+\" #hi#>\"\n","\n","tokenized_input = tokenizer(prompt, return_tensors=\"pt\")\n","input_ids = tokenized_input[\"input_ids\"].cuda()\n","\n","generation_output = model.generate(\n","        input_ids=input_ids,\n","        num_beams=6,\n","        return_dict_in_generate=True,\n","        output_scores=True,\n","        max_new_tokens=130\n",")\n","for seq in generation_output.sequences:\n","    output = tokenizer.decode(seq, skip_special_tokens=True)\n","    print(output.split(\"#hi#>\")[1].strip()) "]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T08:39:08.747361Z","iopub.status.busy":"2024-07-03T08:39:08.746640Z","iopub.status.idle":"2024-07-03T08:39:16.079959Z","shell.execute_reply":"2024-07-03T08:39:16.078150Z","shell.execute_reply.started":"2024-07-03T08:39:08.747332Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["à¤‡à¤¸à¤²à¤¿à¤ à¤‰à¤¨à¥à¤¹à¥‹à¤‚à¤¨à¥‡ à¤­à¤¾à¤°à¤¤\n"]}],"source":["my_text = 'Thus she became the first woman to run for a legislative seat in India.'\n","\n","prompt = my_text+\" #hi#>\"\n","\n","tokenized_input = tokenizer(prompt, return_tensors=\"pt\")\n","input_ids = tokenized_input[\"input_ids\"].cuda()\n","\n","generation_output = model.generate(\n","        input_ids=input_ids,\n","        num_beams=6,\n","        return_dict_in_generate=True,\n","        output_scores=True,\n","        max_new_tokens=130\n",")\n","for seq in generation_output.sequences:\n","    output = tokenizer.decode(seq, skip_special_tokens=True)\n","    print(output.split(\"#hi#>\")[1].strip()) "]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T08:38:58.411000Z","iopub.status.busy":"2024-07-03T08:38:58.410560Z","iopub.status.idle":"2024-07-03T08:38:58.434998Z","shell.execute_reply":"2024-07-03T08:38:58.433820Z","shell.execute_reply.started":"2024-07-03T08:38:58.410968Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'Thus she became the first woman to run for a legislative seat in India. #hi#> à¤‡à¤¸ à¤¤à¤°à¤¹ à¤µà¤¹ à¤­à¤¾à¤°à¤¤ à¤®à¥‡à¤‚ à¤µà¤¿à¤§à¤¾à¤¯à¥€ à¤¸à¥€à¤Ÿ à¤•à¥‡ à¤²à¤¿à¤ à¤¦à¥Œà¤¡à¤¼à¤¨à¥‡ à¤µà¤¾à¤²à¥€ à¤ªà¤¹à¤²à¥€ à¤®à¤¹à¤¿à¤²à¤¾ à¤¬à¤¨à¥€à¤‚à¥¤'"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["dataset_dict['test']['translations'][100]"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T08:35:51.973397Z","iopub.status.busy":"2024-07-03T08:35:51.972566Z","iopub.status.idle":"2024-07-03T08:35:51.980520Z","shell.execute_reply":"2024-07-03T08:35:51.979792Z","shell.execute_reply.started":"2024-07-03T08:35:51.973367Z"},"trusted":true},"outputs":[],"source":["def translator(text, language_code):\n","    prompt = text + \" \" + language_code\n","    tokenized_input = tokenizer(prompt, return_tensors=\"pt\")\n","    input_ids = tokenized_input[\"input_ids\"].cuda()\n","\n","    generation_output = model.generate(\n","        input_ids=input_ids,\n","        num_beams=6,\n","        return_dict_in_generate=True,\n","        output_scores=True,\n","        max_new_tokens=130\n","    )\n","    for seq in generation_output.sequences:\n","        output = tokenizer.decode(seq, skip_special_tokens=True)\n","        return output.split(language_code)[1].strip()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example usage\n","my_text = \"Kerala, a state on India's tropical Malabar Coast\"\n","translated_text_hi = translator(my_text, \"#hi#>\")"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T09:56:59.173613Z","iopub.status.busy":"2024-07-02T09:56:59.173239Z","iopub.status.idle":"2024-07-02T09:56:59.181232Z","shell.execute_reply":"2024-07-02T09:56:59.180287Z","shell.execute_reply.started":"2024-07-02T09:56:59.173584Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'à¤­à¤¾à¤°à¤¤ à¤•à¥€ à¤¤à¤Ÿà¥€à¤¯ à¤®à¤²à¤¬à¤¾à¤° à¤•à¥‹à¤·à¥à¤ '"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["translated_text_hi"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T08:36:03.245856Z","iopub.status.busy":"2024-07-03T08:36:03.245452Z","iopub.status.idle":"2024-07-03T08:36:03.253365Z","shell.execute_reply":"2024-07-03T08:36:03.252024Z","shell.execute_reply.started":"2024-07-03T08:36:03.245823Z"},"trusted":true},"outputs":[],"source":["def translate_texts(translator, dataset, language_code):\n","    tgt_texts, trans_texts = [], []\n","\n","    for translation in dataset['translations']:\n","        src_text, tgt_text = translation.split(language_code)\n","        translated_text = translator(src_text.strip(), language_code)\n","        tgt_texts.append(tgt_text.strip())\n","        trans_texts.append(translated_text)\n","\n","    return tgt_texts, trans_texts"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T08:36:06.047866Z","iopub.status.busy":"2024-07-03T08:36:06.047038Z","iopub.status.idle":"2024-07-03T08:36:19.702721Z","shell.execute_reply":"2024-07-03T08:36:19.701485Z","shell.execute_reply.started":"2024-07-03T08:36:06.047831Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid, fd = os.forkpty()\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Collecting evaluate\n","  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n","Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.20.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\n","Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.1)\n","Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.3.1)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.23.2)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n","Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n","Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: evaluate\n","Successfully installed evaluate-0.4.2\n"]}],"source":["!pip install evaluate"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T08:36:25.681743Z","iopub.status.busy":"2024-07-03T08:36:25.680952Z","iopub.status.idle":"2024-07-03T08:36:39.061744Z","shell.execute_reply":"2024-07-03T08:36:39.060510Z","shell.execute_reply.started":"2024-07-03T08:36:25.681710Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Collecting sacrebleu\n","  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting portalocker (from sacrebleu)\n","  Downloading portalocker-2.10.0-py3-none-any.whl.metadata (8.5 kB)\n","Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2023.12.25)\n","Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.26.4)\n","Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n","Requirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.2.2)\n","Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading portalocker-2.10.0-py3-none-any.whl (18 kB)\n","Installing collected packages: portalocker, sacrebleu\n","Successfully installed portalocker-2.10.0 sacrebleu-2.4.2\n"]}],"source":["!pip install sacrebleu"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T08:39:43.150283Z","iopub.status.busy":"2024-07-03T08:39:43.149612Z","iopub.status.idle":"2024-07-03T08:39:43.524605Z","shell.execute_reply":"2024-07-03T08:39:43.523468Z","shell.execute_reply.started":"2024-07-03T08:39:43.150247Z"},"trusted":true},"outputs":[],"source":["import evaluate\n","import torch"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T10:08:15.960110Z","iopub.status.busy":"2024-07-03T10:08:15.959379Z","iopub.status.idle":"2024-07-03T10:08:16.475825Z","shell.execute_reply":"2024-07-03T10:08:16.474619Z","shell.execute_reply.started":"2024-07-03T10:08:15.960076Z"},"trusted":true},"outputs":[],"source":["from nltk.translate.bleu_score import corpus_bleu\n","\n","def bleu_score2(tgt_texts, trans_texts):\n","    # Prepare the target and translated texts for BLEU calculation\n","    tgt_texts2 = [[tgt_text.split()] for tgt_text in tgt_texts]\n","    trans_text2 = [translated_text.split() for translated_text in trans_texts]\n","\n","    # Calculate BLEU scores for different n-gram weights\n","    bleu_dic = {}\n","    bleu_dic['1-grams'] = corpus_bleu(tgt_texts2, trans_text2, weights=(1.0, 0, 0, 0))\n","    bleu_dic['1-2-grams'] = corpus_bleu(tgt_texts2, trans_text2, weights=(0.5, 0.5, 0, 0))\n","    bleu_dic['1-3-grams'] = corpus_bleu(tgt_texts2, trans_text2, weights=(0.3, 0.3, 0.3, 0))\n","    bleu_dic['1-4-grams'] = corpus_bleu(tgt_texts2, trans_text2, weights=(0.25, 0.25, 0.25, 0.25))\n","\n","    # Calculate the average BLEU score\n","    average_bleu = sum(bleu_dic.values()) / len(bleu_dic)\n","    bleu_dic['average'] = average_bleu\n","\n","    return bleu_dic\n","\n","def bleu_score3(tgt_texts, trans_texts):\n","    # Prepare the target and translated texts for BLEU calculation\n","    tgt_texts2 = [[tgt_text.split()] for tgt_text in tgt_texts]\n","    trans_text2 = [translated_text.split() for translated_text in trans_texts]\n","\n","    # Calculate BLEU scores for different n-gram weights\n","    bleu_dic = {}\n","    bleu_dic['BLEU-1'] = corpus_bleu(tgt_texts2, trans_text2, weights=(1.0, 0, 0, 0))\n","    bleu_dic['BLEU-2'] = corpus_bleu(tgt_texts2, trans_text2, weights=(0.5, 0.5, 0, 0))\n","    bleu_dic['BLEU-3'] = corpus_bleu(tgt_texts2, trans_text2, weights=(0.33, 0.33, 0.34, 0))\n","    bleu_dic['BLEU-4'] = corpus_bleu(tgt_texts2, trans_text2, weights=(0.25, 0.25, 0.25, 0.25))\n","\n","    # Calculate the average BLEU score as per the paper's formula\n","    average_bleu = (bleu_dic['BLEU-1'] + bleu_dic['BLEU-2'] + bleu_dic['BLEU-3'] + bleu_dic['BLEU-4']) / 4\n","    bleu_dic['BLEU_Avg'] = average_bleu * 100  # Multiplying by 100 to match the paper's scaling\n","\n","    return bleu_dic\n"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T10:08:27.128769Z","iopub.status.busy":"2024-07-03T10:08:27.128042Z","iopub.status.idle":"2024-07-03T10:08:27.839715Z","shell.execute_reply":"2024-07-03T10:08:27.838599Z","shell.execute_reply.started":"2024-07-03T10:08:27.128739Z"},"trusted":true},"outputs":[],"source":["bleu_scores2 = bleu_score2(tgt_texts, trans_texts)\n","bleu_scores3 = bleu_score3(tgt_texts, trans_texts)"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T10:08:29.569075Z","iopub.status.busy":"2024-07-03T10:08:29.568281Z","iopub.status.idle":"2024-07-03T10:08:29.576915Z","shell.execute_reply":"2024-07-03T10:08:29.575911Z","shell.execute_reply.started":"2024-07-03T10:08:29.569045Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'1-grams': 0.0018953897948321165,\n"," '1-2-grams': 0.0013165245734676609,\n"," '1-3-grams': 0.0010179381202899296,\n"," '1-4-grams': 0.0005375796660311286,\n"," 'average': 0.0011918580386552089}"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["bleu_scores2"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T10:08:31.581083Z","iopub.status.busy":"2024-07-03T10:08:31.580748Z","iopub.status.idle":"2024-07-03T10:08:31.590080Z","shell.execute_reply":"2024-07-03T10:08:31.589073Z","shell.execute_reply.started":"2024-07-03T10:08:31.581059Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'BLEU-1': 0.0018953897948321165,\n"," 'BLEU-2': 0.0013165245734676609,\n"," 'BLEU-3': 0.0008693741955300347,\n"," 'BLEU-4': 0.0005375796660311286,\n"," 'BLEU_Avg': 0.11547170574652352}"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["bleu_scores3"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T08:41:28.577580Z","iopub.status.busy":"2024-07-03T08:41:28.576806Z","iopub.status.idle":"2024-07-03T10:06:55.836464Z","shell.execute_reply":"2024-07-03T10:06:55.835297Z","shell.execute_reply.started":"2024-07-03T08:41:28.577524Z"},"trusted":true},"outputs":[],"source":["tgt_texts, trans_texts = translate_texts(translator, new_dataset_dict['test'], \"#hi#>\")"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T10:09:02.971060Z","iopub.status.busy":"2024-07-03T10:09:02.970252Z","iopub.status.idle":"2024-07-03T10:09:05.204348Z","shell.execute_reply":"2024-07-03T10:09:05.203290Z","shell.execute_reply.started":"2024-07-03T10:09:02.971030Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"62a7fe69c99d488e8b5b7e4ad68015f8","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/9.01k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import sacrebleu\n","import evaluate\n","\n","# Load evaluation metrics\n","sacrebleu_metric = evaluate.load(\"sacrebleu\")\n","chrf_metric = evaluate.load(\"chrf\")\n","ter_metric = sacrebleu.metrics.TER()\n","\n","# Define the postprocess_text function\n","def postprocess_text(preds, labels):\n","    preds = [pred.strip() for pred in preds]\n","    labels = [label.strip() for label in labels]\n","    return preds, labels\n","\n","# Define the compute_metrics function\n","def compute_metrics(decoded_preds, decoded_labels):\n","    # Post-process the decoded predictions and labels\n","    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n","\n","    # Compute BLEU score\n","    sacrebleu_result = sacrebleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n","    bleu_score = sacrebleu_result['score']\n","\n","    # Compute CHRF score\n","    chrf_result = chrf_metric.compute(predictions=decoded_preds, references=decoded_labels)\n","    chrf_score = chrf_result['score']\n","\n","    # Compute TER score using sacrebleu\n","    ter_score = ter_metric.corpus_score(decoded_preds, [decoded_labels]).score\n","\n","    # Create the result dictionary\n","    result = {\n","        'bleu': round(bleu_score, 4),\n","        'chrf': round(chrf_score, 4),\n","        'ter': round(ter_score, 4)\n","    }\n","    \n","    return result"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T10:09:05.207392Z","iopub.status.busy":"2024-07-03T10:09:05.206684Z","iopub.status.idle":"2024-07-03T10:09:06.059193Z","shell.execute_reply":"2024-07-03T10:09:06.057691Z","shell.execute_reply.started":"2024-07-03T10:09:05.207358Z"},"trusted":true},"outputs":[],"source":["metrics = compute_metrics(tgt_texts, trans_texts)"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T10:09:06.064778Z","iopub.status.busy":"2024-07-03T10:09:06.063780Z","iopub.status.idle":"2024-07-03T10:09:06.072346Z","shell.execute_reply":"2024-07-03T10:09:06.071385Z","shell.execute_reply.started":"2024-07-03T10:09:06.064743Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'bleu': 1.9559, 'chrf': 27.2282, 'ter': 607.4864}"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["metrics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":74,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T13:53:49.345865Z","iopub.status.busy":"2024-07-02T13:53:49.345465Z","iopub.status.idle":"2024-07-02T13:54:03.504352Z","shell.execute_reply":"2024-07-02T13:54:03.503124Z","shell.execute_reply.started":"2024-07-02T13:53:49.345836Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json\n","Model config LlamaConfig {\n","  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 11008,\n","  \"max_position_embeddings\": 4096,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 10000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float16\",\n","  \"transformers_version\": \"4.42.3\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n","loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/model.safetensors.index.json\n","Instantiating LlamaForCausalLM model under default dtype torch.float16.\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2\n","}\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0847dbea1cf6448abbe907e4a2d20b0b","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["All model checkpoint weights were used when initializing LlamaForCausalLM.\n","\n","All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-hf.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n","loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/generation_config.json\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 1,\n","  \"do_sample\": true,\n","  \"eos_token_id\": 2,\n","  \"max_length\": 4096,\n","  \"pad_token_id\": 0,\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.model\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json\n","Model config LlamaConfig {\n","  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 11008,\n","  \"max_position_embeddings\": 4096,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 10000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float16\",\n","  \"transformers_version\": \"4.42.3\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","Uploading the following files to ABHIiiii1/FineTuned-Trans-oneTomany-llama-2-7b: adapter_config.json,README.md,adapter_model.safetensors\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f3c4a4350f324886a8abea949348fa0b","version_major":2,"version_minor":0},"text/plain":["adapter_model.safetensors:   0%|          | 0.00/92.8M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5896a1b50c224e53afd9aadf85d848e4","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["tokenizer config file saved in /tmp/tmp65moef8m/tokenizer_config.json\n","Special tokens file saved in /tmp/tmp65moef8m/special_tokens_map.json\n","Uploading the following files to ABHIiiii1/FineTuned-Trans-oneTomany-llama-2-7b: special_tokens_map.json,tokenizer.json,tokenizer_config.json,tokenizer.model,README.md\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"de9b608164c44370a2e74fbb131e301a","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/ABHIiiii1/FineTuned-Trans-oneTomany-llama-2-7b/commit/5e4d3d208228f25924352f6a9de5cfdbd25cd12b', commit_message='Upload tokenizer', commit_description='', oid='5e4d3d208228f25924352f6a9de5cfdbd25cd12b', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":74,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","from peft import PeftModel\n","\n","# Assuming you have your model and tokenizer already loaded and fine-tuned\n","model_name = \"meta-llama/Llama-2-7b-hf\"\n","base_model = \"meta-llama/Llama-2-7b-hf\"\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=True,\n",")\n","model = AutoModelForCausalLM.from_pretrained(\n","    base_model, device_map={\"\": 0}, quantization_config=bnb_config\n",")\n","tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n","\n","# Load the fine-tuned model\n","model = PeftModel.from_pretrained(model, \"working/results/checkpoint-500/\")\n","\n","# Push the model to Hugging Face Hub\n","model.push_to_hub(\"FineTuned-Trans-oneTomany-llama-2-7b\")\n","tokenizer.push_to_hub(\"FineTuned-Trans-oneTomany-llama-2-7b\")\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T14:15:14.219074Z","iopub.status.busy":"2024-07-02T14:15:14.218381Z","iopub.status.idle":"2024-07-02T14:15:14.223596Z","shell.execute_reply":"2024-07-02T14:15:14.222428Z","shell.execute_reply.started":"2024-07-02T14:15:14.219042Z"},"trusted":true},"outputs":[],"source":["import torch"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T14:18:16.362648Z","iopub.status.busy":"2024-07-02T14:18:16.361903Z","iopub.status.idle":"2024-07-02T14:20:09.344923Z","shell.execute_reply":"2024-07-02T14:20:09.343593Z","shell.execute_reply.started":"2024-07-02T14:18:16.362599Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4a91f0bc402d4e49917ee7b5e8ba1938","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/946 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9a4f33f38dd148c28563dc0065366564","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"14d3c31d7bec4f428b7dd6ec471e85fa","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"17e33516ecce4ba89dd0e6f29f09177b","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"033d1e1043e14f3798105706d82f0722","version_major":2,"version_minor":0},"text/plain":["adapter_config.json:   0%|          | 0.00/671 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ca986a75a013484b93849ddb51cae6df","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c94f9818080e41678a542b819ee79b41","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2ac7f4cad67e44df8550b19a9e4eb79c","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"56368f1b59c64d1ea5b79c1115ed9559","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cfa0e02b301a4ef8bf188d59f9953335","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"45c9a2fabb4d4351984d969f520cc1c1","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f14b8eed28414a318afad1ce1c823eea","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1531fe12966e4e3e8bd376cf745d9c6a","version_major":2,"version_minor":0},"text/plain":["adapter_model.safetensors:   0%|          | 0.00/92.8M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 20.12 MiB is free. Process 2159 has 15.87 GiB memory in use. Of the allocated memory 15.62 GiB is allocated by PyTorch, and 4.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Move the model to GPU if available\u001b[39;00m\n\u001b[1;32m      9\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2796\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2792\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2793\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2794\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2795\u001b[0m         )\n\u001b[0;32m-> 2796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (2 times)]\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 20.12 MiB is free. Process 2159 has 15.87 GiB memory in use. Of the allocated memory 15.62 GiB is allocated by PyTorch, and 4.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","# Load the model and tokenizer from the Hugging Face Hub\n","model_name = \"ABHIiiii1/FineTuned-Trans-oneTomany-llama-2-7b\"  # Replace with your model's path\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(model_name)\n","\n","# Move the model to GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# Set the model to evaluation mode\n","model.eval()\n","\n","def translator(text, language_code):\n","    prompt = text + \" \" + language_code\n","    tokenized_input = tokenizer(prompt, return_tensors=\"pt\")\n","    input_ids = tokenized_input[\"input_ids\"].to(device)\n","\n","    generation_output = model.generate(\n","        input_ids=input_ids,\n","        num_beams=6,\n","        return_dict_in_generate=True,\n","        output_scores=True,\n","        max_new_tokens=130\n","    )\n","    for seq in generation_output.sequences:\n","        output = tokenizer.decode(seq, skip_special_tokens=True)\n","        return output.split(language_code)[1].strip()\n","\n","# Example usage\n","my_text = \"Kerala, a state on India's tropical Malabar Coast\"\n","translated_text_ml = translator(my_text, \"#ml#>\")\n","translated_text_hi = translator(my_text, \"#hi#>\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-01T14:47:59.666899Z","iopub.status.busy":"2024-07-01T14:47:59.666212Z","iopub.status.idle":"2024-07-01T14:49:53.548685Z","shell.execute_reply":"2024-07-01T14:49:53.546629Z","shell.execute_reply.started":"2024-07-01T14:47:59.666864Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"61d5c44222304f59b3eeb97db7c5a7d1","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f10e92555641454fa5ed319ee1a0a4da","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1be052e7a22245df8b83d40c95597040","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5e0f9813b2474674be449f7f045486da","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"746e44297c13480499c8b2ffa46ce307","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"25594b0da6f148c08634093e46bf83c2","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f6ead14e888446acbefe78a9904d88ab","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"247239440f714496a4354754d497d271","version_major":2,"version_minor":0},"text/plain":["adapter_model.safetensors:   0%|          | 0.00/92.8M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 192.12 MiB is free. Process 3238 has 15.71 GiB memory in use. Of the allocated memory 15.45 GiB is allocated by PyTorch, and 5.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Move the model to GPU\u001b[39;00m\n\u001b[1;32m     11\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (5 times)]\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 192.12 MiB is free. Process 3238 has 15.71 GiB memory in use. Of the allocated memory 15.45 GiB is allocated by PyTorch, and 5.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","from peft import PeftModel\n","import torch\n","\n","# Load the model and tokenizer from the Hugging Face Hub\n","model_name = \"ABHIiiii1/FineTuned-Trans-llama-2-7b\"  # Replace with your model's path\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = PeftModel.from_pretrained(AutoModelForCausalLM.from_pretrained(model_name), model_name)\n","\n","# Move the model to GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(\"cuda\")\n","\n","# Set the model to evaluation mode\n","model.eval()\n","\n","# Prepare the input text\n","my_text = \"Kerala, a state on India's tropical Malabar Coast\"\n","prompt = my_text + \" ###>\"\n","\n","# Tokenize the input text\n","tokenized_input = tokenizer(prompt, return_tensors=\"pt\")\n","input_ids = tokenized_input[\"input_ids\"].to(device)\n","\n","# Generate the translation\n","generation_output = model.generate(\n","    input_ids=input_ids,\n","    num_beams=6,\n","    return_dict_in_generate=True,\n","    output_scores=True,\n","    max_new_tokens=130\n",")\n","\n","# Decode and print the generated text\n","for seq in generation_output.sequences:\n","    output = tokenizer.decode(seq, skip_special_tokens=True)\n","    print(output.split(\"###>\")[1].strip())\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5299706,"sourceId":8810847,"sourceType":"datasetVersion"},{"datasetId":5300098,"sourceId":8811391,"sourceType":"datasetVersion"},{"datasetId":5300128,"sourceId":8811427,"sourceType":"datasetVersion"},{"datasetId":5318102,"sourceId":8837322,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
