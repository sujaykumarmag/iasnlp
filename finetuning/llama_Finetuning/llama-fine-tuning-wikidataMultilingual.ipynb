{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8810847,"sourceType":"datasetVersion","datasetId":5299706},{"sourceId":8811391,"sourceType":"datasetVersion","datasetId":5300098},{"sourceId":8811427,"sourceType":"datasetVersion","datasetId":5300128},{"sourceId":8837322,"sourceType":"datasetVersion","datasetId":5318102}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**For singleData**","metadata":{}},{"cell_type":"code","source":"import random\nimport os\n\n# Define paths to your dataset files\nfile_eng_latn_mal_mlym = \"/kaggle/input/wiki-datatrans/wiki/eng_Latn-mal_Mlym/train.eng_Latn\"\nfile_mal_mlym = \"/kaggle/input/wiki-datatrans/wiki/eng_Latn-mal_Mlym/train.mal_Mlym\"\n\n# Function to read and split dataset\ndef split_dataset(file_eng_latn_mal_mlym, file_mal_mlym, train_size, test_size, val_size):\n    with open(file_eng_latn_mal_mlym, 'r', encoding='utf-8') as f_eng, open(file_mal_mlym, 'r', encoding='utf-8') as f_mal:\n        eng_lines = f_eng.readlines()\n        mal_lines = f_mal.readlines()\n        \n        # Combine English and Malayalam lines into pairs\n        data = list(zip(eng_lines, mal_lines))\n        \n        # Shuffle the data\n        random.shuffle(data)\n        \n        # Split into train, test, and validation sets\n        train_data = data[:train_size]\n        test_data = data[train_size:train_size + test_size]\n        val_data = data[train_size + test_size:train_size + test_size + val_size]\n        \n        return train_data, test_data, val_data\n\n# Define the number of samples for train, test, and validation\ntrain_size = 30000\ntest_size = 2000\nval_size = 2000\n\n# Split the dataset\ntrain_data, test_data, val_data = split_dataset(file_eng_latn_mal_mlym, file_mal_mlym, train_size, test_size, val_size)\n\n# Output the first few samples of each split for verification\nprint(f\"Number of training samples: {len(train_data)}\")\nprint(f\"Number of testing samples: {len(test_data)}\")\nprint(f\"Number of validation samples: {len(val_data)}\")\n\n# Optionally, you can write these splits to new files if needed\n# Example:\noutput_dir = \"/path/to/output/directory\"\nos.makedirs(output_dir, exist_ok=True)\n\ndef write_dataset(data, output_file_eng, output_file_mal):\n    with open(output_file_eng, 'w', encoding='utf-8') as f_eng, open(output_file_mal, 'w', encoding='utf-8') as f_mal:\n        for eng, mal in data:\n            f_eng.write(eng)\n            f_mal.write(mal)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\n\ntrain_dataset = Dataset.from_dict({'translations': train_data})\nval_dataset = Dataset.from_dict({'translations': val_data})\ntest_dataset = Dataset.from_dict({'translations': test_data})\n\n# Create DatasetDict\ndataset_dict = DatasetDict({\n    'train': train_dataset,\n    'validation': val_dataset,\n    'test': test_dataset\n})\n\n# Print out the dataset_dict structure\nprint(dataset_dict)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**For MultipleData**","metadata":{}},{"cell_type":"code","source":"file_eng_latn_mal_mlym = \"/kaggle/input/wiki-datatrans/wiki/eng_Latn-mal_Mlym/train.eng_Latn\"\nfile_mal_mlym = \"/kaggle/input/wiki-datatrans/wiki/eng_Latn-mal_Mlym/train.mal_Mlym\"\n\nwith open(file_eng_latn_mal_mlym, 'r', encoding='utf-8') as f_eng_latn, open(file_mal_mlym, 'r', encoding='utf-8') as f_mal_mlym:\n    eng_latn_mal_mlym_data = f_eng_latn.readlines()\n    mal_mlym_data = f_mal_mlym.readlines()\n\n# Combine into one dataset\neng_latn_mal_mlym_pairs = list(zip(eng_latn_mal_mlym_data, mal_mlym_data))\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:29:33.515660Z","iopub.execute_input":"2024-07-02T05:29:33.516263Z","iopub.status.idle":"2024-07-02T05:29:33.846203Z","shell.execute_reply.started":"2024-07-02T05:29:33.516229Z","shell.execute_reply":"2024-07-02T05:29:33.845423Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"file_eng_latn_tam_taml = \"/kaggle/input/wiki-datatrans/wiki/eng_Latn-hin_Deva/train.eng_Latn\"\nfile_tam_taml = \"/kaggle/input/wiki-datatrans/wiki/eng_Latn-hin_Deva/train.hin_Deva\"\n\nwith open(file_eng_latn_tam_taml, 'r', encoding='utf-8') as f_eng_latn_tam_taml, open(file_tam_taml, 'r', encoding='utf-8') as f_tam_taml:\n    eng_latn_tam_taml_data = f_eng_latn_tam_taml.readlines()\n    tam_taml_data = f_tam_taml.readlines()\n\n# Combine into one dataset\neng_latn_tam_taml_pairs = list(zip(eng_latn_tam_taml_data, tam_taml_data))\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:29:34.707211Z","iopub.execute_input":"2024-07-02T05:29:34.707888Z","iopub.status.idle":"2024-07-02T05:29:35.022142Z","shell.execute_reply.started":"2024-07-02T05:29:34.707857Z","shell.execute_reply":"2024-07-02T05:29:35.021417Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Format English to Malayalam dataset\nformatted_eng_mal_dataset = [\n    f\"{eng.strip()} #ml#> {mal.strip()}\" for eng, mal in eng_latn_mal_mlym_pairs\n]\n\n# Format English to Hindi dataset\nformatted_eng_hi_dataset = [\n    f\"{eng.strip()} #hi#> {tam.strip()}\" for eng, tam in eng_latn_tam_taml_pairs\n]\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:29:35.858140Z","iopub.execute_input":"2024-07-02T05:29:35.858507Z","iopub.status.idle":"2024-07-02T05:29:35.935905Z","shell.execute_reply.started":"2024-07-02T05:29:35.858477Z","shell.execute_reply":"2024-07-02T05:29:35.935121Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import random\n\n# Combine the datasets\ncombined_dataset = formatted_eng_mal_dataset + formatted_eng_hi_dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:29:36.807966Z","iopub.execute_input":"2024-07-02T05:29:36.808335Z","iopub.status.idle":"2024-07-02T05:29:36.814178Z","shell.execute_reply.started":"2024-07-02T05:29:36.808304Z","shell.execute_reply":"2024-07-02T05:29:36.813295Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import random\n\n# # Shuffle the combined dataset\n# random.shuffle(combined_dataset)\n\n# Define the number of examples for each split\ntrain_size = 30000\ntest_size = 2000\nvalidation_size = 2000\n\n# Initialize counters for each category (#ml#> and #hi#>)\nml_count_train = 0\nhi_count_train = 0\nml_count_test = 0\nhi_count_test = 0\nml_count_val = 0\nhi_count_val = 0\n\n# Initialize lists for train, test, and validation datasets\ntrain_dataset = []\ntest_dataset = []\nvalidation_dataset = []\n\n# Iterate through the combined dataset\nfor pair in combined_dataset:\n    if '#ml#>' in pair:\n        if ml_count_train < train_size / 2:\n            train_dataset.append(pair)\n            ml_count_train += 1\n        elif ml_count_test < test_size / 2:\n            test_dataset.append(pair)\n            ml_count_test += 1\n        elif ml_count_val < validation_size / 2:\n            validation_dataset.append(pair)\n            ml_count_val += 1\n    elif '#hi#>' in pair:\n        if hi_count_train < train_size / 2:\n            train_dataset.append(pair)\n            hi_count_train += 1\n        elif hi_count_test < test_size / 2:\n            test_dataset.append(pair)\n            hi_count_test += 1\n        elif hi_count_val < validation_size / 2:\n            validation_dataset.append(pair)\n            hi_count_val += 1\n\n# Verify the sizes of each dataset\nprint(f\"Train Dataset Size: {len(train_dataset)}\")\nprint(f\"Test Dataset Size: {len(test_dataset)}\")\nprint(f\"Validation Dataset Size: {len(validation_dataset)}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:29:39.239448Z","iopub.execute_input":"2024-07-02T05:29:39.239792Z","iopub.status.idle":"2024-07-02T05:29:39.312299Z","shell.execute_reply.started":"2024-07-02T05:29:39.239758Z","shell.execute_reply":"2024-07-02T05:29:39.311412Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Train Dataset Size: 30000\nTest Dataset Size: 2000\nValidation Dataset Size: 2000\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\n\n# Combine the datasets into a single list of dictionaries\ncombined_data = {\n    \"translations\": train_dataset + validation_dataset + test_dataset\n}\n\n# Define lengths for each split\ntrain_length = len(train_dataset)\nvalidation_length = len(validation_dataset)\ntest_length = len(test_dataset)\n\n# Create DatasetDict\ndataset_dict = DatasetDict({\n    \"train\": Dataset.from_dict(combined_data).select(range(train_length)),\n    \"validation\": Dataset.from_dict(combined_data).select(range(train_length, train_length + validation_length)),\n    \"test\": Dataset.from_dict(combined_data).select(range(train_length + validation_length, train_length + validation_length + test_length)),\n})\n\n# Print the structure and sizes of the DatasetDict\nprint(dataset_dict)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:29:40.581008Z","iopub.execute_input":"2024-07-02T05:29:40.581608Z","iopub.status.idle":"2024-07-02T05:29:42.592427Z","shell.execute_reply.started":"2024-07-02T05:29:40.581576Z","shell.execute_reply":"2024-07-02T05:29:42.591473Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['translations'],\n        num_rows: 30000\n    })\n    validation: Dataset({\n        features: ['translations'],\n        num_rows: 2000\n    })\n    test: Dataset({\n        features: ['translations'],\n        num_rows: 2000\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"# from datasets import DatasetDict\n# # Shuffle only train and validation datasets\n# dataset_dict['train'] = dataset_dict['train'].shuffle(seed=42)\n# dataset_dict['validation'] = dataset_dict['validation'].shuffle(seed=42)\n\n# print(dataset_dict)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_dict['validation']['translations'][-1]","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:29:48.925047Z","iopub.execute_input":"2024-07-02T05:29:48.925920Z","iopub.status.idle":"2024-07-02T05:29:48.937975Z","shell.execute_reply.started":"2024-07-02T05:29:48.925888Z","shell.execute_reply":"2024-07-02T05:29:48.936969Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'The oldest reference to Rajasthan is found in a stone inscription dated back to 625 CE. #hi#> राजस्थान का सबसे प्राचीन संदर्भ 625 ईस्वी के एक पत्थर के शिलालेख में पाया गया है।'"},"metadata":{}}]},{"cell_type":"code","source":"dataset_dict['validation']['translations'][1]","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:29:56.556783Z","iopub.execute_input":"2024-07-02T05:29:56.557138Z","iopub.status.idle":"2024-07-02T05:29:56.568957Z","shell.execute_reply.started":"2024-07-02T05:29:56.557109Z","shell.execute_reply":"2024-07-02T05:29:56.568053Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'See if you can detect any differences between the notes by sight. #ml#> നോട്ടുകൾ തമ്മിൽ കാഴ്ചയില്\\u200d എന്തെങ്കിലും വ്യത്യാസം ഉണ്ടോയെന്ന് നോക്കൂ.'"},"metadata":{}}]},{"cell_type":"code","source":"dataset_dict_shuffled = dataset_dict.shuffle(seed=42)\n\nprint(dataset_dict_shuffled)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:29:59.452233Z","iopub.execute_input":"2024-07-02T05:29:59.452587Z","iopub.status.idle":"2024-07-02T05:29:59.486000Z","shell.execute_reply.started":"2024-07-02T05:29:59.452561Z","shell.execute_reply":"2024-07-02T05:29:59.485112Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['translations'],\n        num_rows: 30000\n    })\n    validation: Dataset({\n        features: ['translations'],\n        num_rows: 2000\n    })\n    test: Dataset({\n        features: ['translations'],\n        num_rows: 2000\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset_dict_shuffled['test']['translations'][-2]","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:30:04.893842Z","iopub.execute_input":"2024-07-02T05:30:04.894211Z","iopub.status.idle":"2024-07-02T05:30:04.918586Z","shell.execute_reply.started":"2024-07-02T05:30:04.894179Z","shell.execute_reply":"2024-07-02T05:30:04.917700Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"\"Check with the central bank of any country you're visiting for information on serial numbers. #ml#> സീരിയൽ നമ്പറുകളെക്കുറിച്ചുള്ള വിവരങ്ങൾക്കായി നിങ്ങൾ സന്ദർശിക്കുന്ന ഏതൊരു രാജ്യത്തിന്\\u200dറെയും കേന്ദ്ര ബാങ്കുമായി ബന്ധപ്പെടുക.\""},"metadata":{}}]},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\n\n# Assume dataset_dict_shuffled is already defined and shuffled\n\n# Filter out English to Hindi pairs (#hi#>) from the 'test' split\ndef filter_hi(example):\n    return '#hi#>' in example['translations']\n\n# Apply the filter to the 'test' split\ntest_hi_pairs = dataset_dict_shuffled['test'].filter(filter_hi)\n\n# Create a new DatasetDict for the filtered pairs\ndataset_dict_hi_test = DatasetDict({\n    \"test_hi\": test_hi_pairs\n})\n\n# Print the structure and size of the new DatasetDict\nprint(dataset_dict_hi_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:48:08.145343Z","iopub.execute_input":"2024-07-02T09:48:08.145718Z","iopub.status.idle":"2024-07-02T09:48:08.198378Z","shell.execute_reply.started":"2024-07-02T09:48:08.145688Z","shell.execute_reply":"2024-07-02T09:48:08.197187Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aba828538f7148b58521fef8bb651e59"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    test_hi: Dataset({\n        features: ['translations'],\n        num_rows: 1000\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset_dict_hi_test['test_hi']['translations'][4]","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:48:30.522852Z","iopub.execute_input":"2024-07-02T09:48:30.523200Z","iopub.status.idle":"2024-07-02T09:48:30.543310Z","shell.execute_reply.started":"2024-07-02T09:48:30.523172Z","shell.execute_reply":"2024-07-02T09:48:30.542399Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"'Wellness checks are routine medical examinations that determine a dog’s overall health. #hi#> स्वास्थ्य की जांच नियमित स्वास्थ्य परीक्षण है जो कुत्ते के समस्त स्वास्थ्य को निर्धारित करती है।'"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\n\n# Assume dataset_dict_shuffled is already defined and shuffled\n\n# Filter out English to Malayalam pairs (#ml#>) from the 'test' split\ndef filter_ml(example):\n    return '#ml#>' in example['translations']\n\n# Apply the filter to the 'test' split\ntest_ml_pairs = dataset_dict_shuffled['test'].filter(filter_ml)\n\n# Create a new DatasetDict for the filtered pairs\ndataset_dict_ml_test = DatasetDict({\n    \"test_ml\": test_ml_pairs\n})\n\n# Print the structure and size of the new DatasetDict\nprint(dataset_dict_ml_test)\n\n# Accessing information about the new DatasetDict\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:46:31.357205Z","iopub.execute_input":"2024-07-02T09:46:31.357866Z","iopub.status.idle":"2024-07-02T09:46:31.412353Z","shell.execute_reply.started":"2024-07-02T09:46:31.357834Z","shell.execute_reply":"2024-07-02T09:46:31.411300Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ae73f1b4e084b7995e25fe855dc3b28"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    test_ml: Dataset({\n        features: ['translations'],\n        num_rows: 1000\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset_dict_ml_test['test_ml']['translations'][-4]","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:47:05.620007Z","iopub.execute_input":"2024-07-02T09:47:05.620737Z","iopub.status.idle":"2024-07-02T09:47:05.639134Z","shell.execute_reply.started":"2024-07-02T09:47:05.620703Z","shell.execute_reply":"2024-07-02T09:47:05.638184Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"\"Surfaces that aren't slate can warp as they age while a slate surface will stay level and won't wear down. #ml#> ഒരു സ്ലേറ്റ് ഉപരിതലം കേടുവരാതെ നിരപ്പായി തന്നെ നിലകൊള്ളുമ്പോൾ സ്ലേറ്റ് അല്ലാത്ത ഉപരിതലങ്ങൾ കാലം കൂടുന്നതിനനുസരിച്ച് കേടുവരാൻ സാധ്യതയുണ്ട്.\""},"metadata":{}}]},{"cell_type":"code","source":"\n\n# # Shuffle the combined dataset\n# random.shuffle(combined_dataset)\n\n# Split the dataset\ntotal_size = len(combined_dataset)\ntrain_size = int(0.8 * total_size)\nval_size = int(0.1 * total_size)\ntest_size = total_size - train_size - val_size\n\ntrain_data = combined_dataset[:train_size]\nval_data = combined_dataset[train_size:train_size + val_size]\ntest_data = combined_dataset[train_size + val_size:]\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:14:22.131593Z","iopub.execute_input":"2024-07-02T05:14:22.132521Z","iopub.status.idle":"2024-07-02T05:14:22.157925Z","shell.execute_reply.started":"2024-07-02T05:14:22.132488Z","shell.execute_reply":"2024-07-02T05:14:22.157045Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_dataset[9]","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:14:41.230314Z","iopub.execute_input":"2024-07-02T05:14:41.230698Z","iopub.status.idle":"2024-07-02T05:14:41.236491Z","shell.execute_reply.started":"2024-07-02T05:14:41.230670Z","shell.execute_reply":"2024-07-02T05:14:41.235633Z"},"trusted":true},"execution_count":114,"outputs":[{"execution_count":114,"output_type":"execute_result","data":{"text/plain":"'Lab tests conducted by Panat and Varanasi showed that the drop in energy output from the panels is steep, occurs at the very beginning of the process of dust accumulation, and can easily mark a 30% reduction in output after a month without cleaning. Even a 1% reduction in power, for a 150-megawatt solar installation, they calculated, could result in a $200,000 loss in annual revenue. #ml#> പാനലുകളുടെ ഊർജ്ജ ഉൽപ്പാദനം വേഗത്തിൽ കുറയുന്നു, പൊടി അടിഞ്ഞുകൂടാൻ തുടങ്ങുമ്പോൾ തന്നെ കുറയാൻ തുടങ്ങുന്നു, വൃത്തിയാക്കാതെ തന്നെ ഒരു മാസത്തിന് ശേഷം എളുപ്പത്തിൽ 30% കുറയും എന്ന് ലബോറട്ടറി പഠനങ്ങളിൽ നിന്ന് പാനറ്റും വാരണാസിയും കണ്ടെത്തി. 150 മെഗാവാട്ട് സോളാർ ഇൻസ്റ്റാളേഷനായി വൈദ്യുതിയിൽ 1% കുറവുണ്ടായാൽ പോലും, വാർഷിക വരുമാനത്തിൽ $200,000 നഷ്ടം സംഭവിക്കുമെന്ന് അവർ കണക്കാക്കി.'"},"metadata":{}}]},{"cell_type":"code","source":"def save_dataset(data, file_path):\n    with open(file_path, 'w', encoding='utf-8') as f:\n        f.write(\"\\n\".join(data))\n\nsave_dataset(train_data, \"train_dataset.txt\")\nsave_dataset(val_data, \"val_dataset.txt\")\nsave_dataset(test_data, \"test_dataset.txt\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T04:32:37.065603Z","iopub.execute_input":"2024-07-02T04:32:37.066452Z","iopub.status.idle":"2024-07-02T04:32:37.180194Z","shell.execute_reply.started":"2024-07-02T04:32:37.066419Z","shell.execute_reply":"2024-07-02T04:32:37.179159Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\n\ndef load_dataset_from_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n    return Dataset.from_dict({\"text\": [line.strip() for line in lines]})\n\ntrain_dataset = load_dataset_from_file(\"train_dataset.txt\")\nval_dataset = load_dataset_from_file(\"val_dataset.txt\")\ntest_dataset = load_dataset_from_file(\"test_dataset.txt\")\n\ndataset_dict = DatasetDict({\n    \"train\": train_dataset,\n    \"validation\": val_dataset,\n    \"test\": test_dataset\n})\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T04:32:38.206768Z","iopub.execute_input":"2024-07-02T04:32:38.207592Z","iopub.status.idle":"2024-07-02T04:32:38.793800Z","shell.execute_reply.started":"2024-07-02T04:32:38.207561Z","shell.execute_reply":"2024-07-02T04:32:38.793034Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"dataset_dict","metadata":{"execution":{"iopub.status.busy":"2024-07-02T04:32:39.260876Z","iopub.execute_input":"2024-07-02T04:32:39.261247Z","iopub.status.idle":"2024-07-02T04:32:39.268573Z","shell.execute_reply.started":"2024-07-02T04:32:39.261215Z","shell.execute_reply":"2024-07-02T04:32:39.267579Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 65512\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 8189\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 8190\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\n\ndef load_dataset_from_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n    return Dataset.from_dict({\"translations\": [line.strip() for line in lines]})\n\ntrain_dataset = load_dataset_from_file(\"train_dataset.txt\")\nval_dataset = load_dataset_from_file(\"val_dataset.txt\")\ntest_dataset = load_dataset_from_file(\"test_dataset.txt\")\n\ndataset_dict = DatasetDict({\n    \"train\": train_dataset,\n    \"validation\": val_dataset,\n    \"test\": test_dataset\n})\n\nprint(dataset_dict)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T04:32:40.538233Z","iopub.execute_input":"2024-07-02T04:32:40.538663Z","iopub.status.idle":"2024-07-02T04:32:41.035990Z","shell.execute_reply.started":"2024-07-02T04:32:40.538630Z","shell.execute_reply":"2024-07-02T04:32:41.035061Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['translations'],\n        num_rows: 65512\n    })\n    validation: Dataset({\n        features: ['translations'],\n        num_rows: 8189\n    })\n    test: Dataset({\n        features: ['translations'],\n        num_rows: 8190\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset_dict['train'][9000]","metadata":{"execution":{"iopub.status.busy":"2024-07-02T04:38:23.281143Z","iopub.execute_input":"2024-07-02T04:38:23.282032Z","iopub.status.idle":"2024-07-02T04:38:23.288291Z","shell.execute_reply.started":"2024-07-02T04:38:23.282000Z","shell.execute_reply":"2024-07-02T04:38:23.287377Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"{'translations': 'It is ritually the most valued species in the state. #mar#> ही राज्यातील विधीभूत सर्वात मौल्यवान प्रजाती आहे.'}"},"metadata":{}}]},{"cell_type":"code","source":"dataset_dict['train'][-8700]","metadata":{"execution":{"iopub.status.busy":"2024-07-02T04:38:25.421310Z","iopub.execute_input":"2024-07-02T04:38:25.421735Z","iopub.status.idle":"2024-07-02T04:38:25.428146Z","shell.execute_reply.started":"2024-07-02T04:38:25.421703Z","shell.execute_reply":"2024-07-02T04:38:25.427259Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"{'translations': \"Sandhyakar Nandi's semi-fictional epic Ramacharitam (12th century) is an important source of Pala history. #hi#> संध्याकर नंदी का अर्ध-काल्पनिक महाकाव्य रामचरितमानस (12वीं शताब्दी) पाल इतिहास का एक महत्वपूर्ण स्रोत है।\"}"},"metadata":{}}]},{"cell_type":"code","source":"dataset_dict['test'][-8189]","metadata":{"execution":{"iopub.status.busy":"2024-07-02T04:44:21.843128Z","iopub.execute_input":"2024-07-02T04:44:21.843999Z","iopub.status.idle":"2024-07-02T04:44:21.850150Z","shell.execute_reply.started":"2024-07-02T04:44:21.843961Z","shell.execute_reply":"2024-07-02T04:44:21.849226Z"},"trusted":true},"execution_count":74,"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"{'translations': 'There have been attempts at conservation and reforestation. #hi#> वहाँ संरक्षण और वनीकरण के प्रयास किए गए हैं।'}"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n\n# Convert the Hugging Face datasets to Pandas DataFrames\ntrain_df = pd.DataFrame(dataset_dict['train'][\"translations\"], columns=[\"translations\"])\ntest_df = pd.DataFrame(dataset_dict['test'][\"translations\"], columns=[\"translations\"])\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T04:38:33.766575Z","iopub.execute_input":"2024-07-02T04:38:33.766934Z","iopub.status.idle":"2024-07-02T04:38:33.987128Z","shell.execute_reply.started":"2024-07-02T04:38:33.766907Z","shell.execute_reply":"2024-07-02T04:38:33.986211Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"len(train_df)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T04:42:15.996865Z","iopub.execute_input":"2024-07-02T04:42:15.997253Z","iopub.status.idle":"2024-07-02T04:42:16.003267Z","shell.execute_reply.started":"2024-07-02T04:42:15.997222Z","shell.execute_reply":"2024-07-02T04:42:16.002325Z"},"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"75694"},"metadata":{}}]},{"cell_type":"code","source":"len(test_df)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T04:42:26.546778Z","iopub.execute_input":"2024-07-02T04:42:26.547140Z","iopub.status.idle":"2024-07-02T04:42:26.552992Z","shell.execute_reply.started":"2024-07-02T04:42:26.547110Z","shell.execute_reply":"2024-07-02T04:42:26.552027Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"9463"},"metadata":{}}]},{"cell_type":"code","source":"test_df['translations'][9462]","metadata":{"execution":{"iopub.status.busy":"2024-07-02T04:43:45.856252Z","iopub.execute_input":"2024-07-02T04:43:45.856962Z","iopub.status.idle":"2024-07-02T04:43:45.862504Z","shell.execute_reply.started":"2024-07-02T04:43:45.856928Z","shell.execute_reply":"2024-07-02T04:43:45.861685Z"},"trusted":true},"execution_count":73,"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"'He was followed by his two sons who became kings in succession. #hi#> उनके बाद उनके दो बेटे हुए जो लगातार राजा बने।'"},"metadata":{}}]},{"cell_type":"code","source":"# Select 15,000 examples from train for English to Malayalam\ntrain_ml = train_df[train_df['translations'].str.contains('#ml#>')].sample(n=15000, random_state=42)\n\n# Select 15,000 examples from train for English to Hindi\ntrain_hi = train_df[train_df['translations'].str.contains('#hi#>')].sample(n=15000, random_state=42)\n\n# Select 1,000 examples from test for English to Malayalam\ntest_ml = test_df[test_df['translations'].str.contains('#ml#>')].sample(n=1000, random_state=42)\n\n# Select 1,000 examples from test for English to Hindi\ntest_hi = test_df[test_df['translations'].str.contains('#hi#>')].sample(n=1000, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T04:38:34.508221Z","iopub.execute_input":"2024-07-02T04:38:34.508606Z","iopub.status.idle":"2024-07-02T04:38:34.643294Z","shell.execute_reply.started":"2024-07-02T04:38:34.508574Z","shell.execute_reply":"2024-07-02T04:38:34.642069Z"},"trusted":true},"execution_count":63,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[63], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Select 15,000 examples from train for English to Malayalam\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_ml \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtranslations\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontains\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m#ml#>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Select 15,000 examples from train for English to Hindi\u001b[39;00m\n\u001b[1;32m      5\u001b[0m train_hi \u001b[38;5;241m=\u001b[39m train_df[train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslations\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#hi#>\u001b[39m\u001b[38;5;124m'\u001b[39m)]\u001b[38;5;241m.\u001b[39msample(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15000\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/generic.py:6118\u001b[0m, in \u001b[0;36mNDFrame.sample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[0m\n\u001b[1;32m   6115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   6116\u001b[0m     weights \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mpreprocess_weights(\u001b[38;5;28mself\u001b[39m, weights, axis)\n\u001b[0;32m-> 6118\u001b[0m sampled_indices \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6119\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(sampled_indices, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   6121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/sample.py:152\u001b[0m, in \u001b[0;36msample\u001b[0;34m(obj_len, size, replace, weights, random_state)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weights: weights sum to zero\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\n\u001b[1;32m    153\u001b[0m     np\u001b[38;5;241m.\u001b[39mintp, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    154\u001b[0m )\n","File \u001b[0;32mnumpy/random/mtrand.pyx:945\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: a must be greater than 0 unless no samples are taken"],"ename":"ValueError","evalue":"a must be greater than 0 unless no samples are taken","output_type":"error"}]},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming you have already loaded train and test datasets into DataFrames train_df and test_df\n\n# Select 15,000 examples from train for English to Malayalam\ntrain_ml = train_df[train_df['translations'].str.contains('#ml#>')].sample(n=15000, random_state=42)\n\n# Select 15,000 examples from train for English to Hindi\ntrain_hi = train_df[train_df['translations'].str.contains('#hi#>')].sample(n=15000, random_state=42)\n\n# Select 1,000 examples from test for English to Malayalam if available\nif len(test_df[test_df['translations'].str.contains('#ml#>')]) > 1000:\n    test_ml = test_df[test_df['translations'].str.contains('#ml#>')].sample(n=1000, random_state=42)\nelse:\n    test_ml = test_df[test_df['translations'].str.contains('#ml#>')]\n\n# Select 1,000 examples from test for English to Hindi if available\nif len(test_df[test_df['translations'].str.contains('#hi#>')]) > 1000:\n    test_hi = test_df[test_df['translations'].str.contains('#hi#>')].sample(n=1000, random_state=42)\nelse:\n    test_hi = test_df[test_df['translations'].str.contains('#hi#>')]\n\nprint(\"Train ML shape:\", train_ml.shape)\nprint(\"Train HI shape:\", train_hi.shape)\nprint(\"Test ML shape:\", test_ml.shape)\nprint(\"Test HI shape:\", test_hi.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T04:34:16.236253Z","iopub.execute_input":"2024-07-02T04:34:16.237121Z","iopub.status.idle":"2024-07-02T04:34:16.384124Z","shell.execute_reply.started":"2024-07-02T04:34:16.237091Z","shell.execute_reply":"2024-07-02T04:34:16.383233Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"Train ML shape: (15000, 1)\nTrain HI shape: (15000, 1)\nTest ML shape: (0, 1)\nTest HI shape: (1000, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\n\n# Combine the selected data\nselected_train_data = pd.concat([train_ml, train_hi])\nselected_test_data = pd.concat([test_ml, test_hi])\n\n# Create new Dataset objects\nselected_train_dataset = Dataset.from_pandas(selected_train_data)\nselected_test_dataset = Dataset.from_pandas(selected_test_data)\n\n# Create new DatasetDict\nselected_dataset_dict = DatasetDict({\n    \"train\": selected_train_dataset,\n    \"test\": selected_test_dataset\n})\n\nprint(selected_dataset_dict)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T04:27:10.044754Z","iopub.execute_input":"2024-07-02T04:27:10.045712Z","iopub.status.idle":"2024-07-02T04:27:10.187633Z","shell.execute_reply.started":"2024-07-02T04:27:10.045678Z","shell.execute_reply":"2024-07-02T04:27:10.186652Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['translations', '__index_level_0__'],\n        num_rows: 30000\n    })\n    test: Dataset({\n        features: ['translations', '__index_level_0__'],\n        num_rows: 2000\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import DatasetDict\n\n# Assuming selected_dataset_dict is already defined as in your previous messages\n\n# Remove the '__index_level_0__' column from train dataset\nselected_dataset_dict['train'] = selected_dataset_dict['train'].remove_columns('__index_level_0__')\n\n# Remove the '__index_level_0__' column from test dataset\nselected_dataset_dict['test'] = selected_dataset_dict['test'].remove_columns('__index_level_0__')\n\nprint(selected_dataset_dict)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T04:27:12.915038Z","iopub.execute_input":"2024-07-02T04:27:12.915394Z","iopub.status.idle":"2024-07-02T04:27:12.924319Z","shell.execute_reply.started":"2024-07-02T04:27:12.915367Z","shell.execute_reply":"2024-07-02T04:27:12.923447Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['translations'],\n        num_rows: 30000\n    })\n    test: Dataset({\n        features: ['translations'],\n        num_rows: 2000\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import DatasetDict\n\n# Assuming selected_dataset_dict is already defined as in your previous messages\n\n# Shuffle the train dataset\nselected_dataset_dict['train'] = selected_dataset_dict['train'].shuffle()\n\n# Shuffle the test dataset\nselected_dataset_dict['test'] = selected_dataset_dict['test'].shuffle()\n\nprint(selected_dataset_dict)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T04:27:16.831352Z","iopub.execute_input":"2024-07-02T04:27:16.832234Z","iopub.status.idle":"2024-07-02T04:27:16.866220Z","shell.execute_reply.started":"2024-07-02T04:27:16.832199Z","shell.execute_reply":"2024-07-02T04:27:16.865295Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['translations'],\n        num_rows: 30000\n    })\n    test: Dataset({\n        features: ['translations'],\n        num_rows: 2000\n    })\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# SetUp Directory","metadata":{}},{"cell_type":"code","source":"%pwd","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:45:27.190112Z","iopub.execute_input":"2024-06-29T08:45:27.190458Z","iopub.status.idle":"2024-06-29T08:45:27.205435Z","shell.execute_reply.started":"2024-06-29T08:45:27.190429Z","shell.execute_reply":"2024-06-29T08:45:27.204537Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working'"},"metadata":{}}]},{"cell_type":"code","source":"%cd ..","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:45:29.184043Z","iopub.execute_input":"2024-06-29T08:45:29.184387Z","iopub.status.idle":"2024-06-29T08:45:29.190500Z","shell.execute_reply.started":"2024-06-29T08:45:29.184358Z","shell.execute_reply":"2024-06-29T08:45:29.189552Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle\n","output_type":"stream"}]},{"cell_type":"code","source":"%ls","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:45:30.119916Z","iopub.execute_input":"2024-06-29T08:45:30.120553Z","iopub.status.idle":"2024-06-29T08:45:31.062110Z","shell.execute_reply.started":"2024-06-29T08:45:30.120518Z","shell.execute_reply":"2024-06-29T08:45:31.061131Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[0m\u001b[01;34minput\u001b[0m/  \u001b[01;34mlib\u001b[0m/  \u001b[01;34mworking\u001b[0m/\n","output_type":"stream"}]},{"cell_type":"code","source":"# %mkdir working/results/\n![ ! -d working/results/ ] && mkdir -p working/results/","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:30:19.922181Z","iopub.execute_input":"2024-07-02T05:30:19.922841Z","iopub.status.idle":"2024-07-02T05:30:20.863152Z","shell.execute_reply.started":"2024-07-02T05:30:19.922811Z","shell.execute_reply":"2024-07-02T05:30:20.862053Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Necessary Installs and Imports","metadata":{}},{"cell_type":"markdown","source":"## Installs","metadata":{}},{"cell_type":"code","source":"!pip install -U datasets transformers trl accelerate peft bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-07-02T14:15:52.336200Z","iopub.execute_input":"2024-07-02T14:15:52.336563Z","iopub.status.idle":"2024-07-02T14:16:37.092336Z","shell.execute_reply.started":"2024-07-02T14:15:52.336531Z","shell.execute_reply":"2024-07-02T14:16:37.091394Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.2)\nCollecting datasets\n  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nCollecting transformers\n  Downloading transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting trl\n  Downloading trl-0.9.4-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\nCollecting accelerate\n  Downloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\nCollecting peft\n  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nCollecting pyarrow>=15.0.0 (from datasets)\n  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.1)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.3.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.23.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl) (2.1.2)\nCollecting tyro>=0.5.11 (from trl)\n  Downloading tyro-0.8.5-py3-none-any.whl.metadata (8.2 kB)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.1.2)\nCollecting docstring-parser>=0.16 (from tyro>=0.5.11->trl)\n  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (13.7.0)\nCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.17.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\nDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.42.3-py3-none-any.whl (9.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading trl-0.9.4-py3-none-any.whl (226 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tyro-0.8.5-py3-none-any.whl (103 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\nDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nInstalling collected packages: shtab, pyarrow, docstring-parser, tyro, bitsandbytes, accelerate, transformers, datasets, trl, peft\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 14.0.2\n    Uninstalling pyarrow-14.0.2:\n      Successfully uninstalled pyarrow-14.0.2\n  Attempting uninstall: docstring-parser\n    Found existing installation: docstring-parser 0.15\n    Uninstalling docstring-parser-0.15:\n      Successfully uninstalled docstring-parser-0.15\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.30.1\n    Uninstalling accelerate-0.30.1:\n      Successfully uninstalled accelerate-0.30.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.41.2\n    Uninstalling transformers-4.41.2:\n      Successfully uninstalled transformers-4.41.2\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.19.2\n    Uninstalling datasets-2.19.2:\n      Successfully uninstalled datasets-2.19.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.1 which is incompatible.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ncudf 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-0.31.0 bitsandbytes-0.43.1 datasets-2.20.0 docstring-parser-0.16 peft-0.11.1 pyarrow-16.1.0 shtab-1.7.1 transformers-4.42.3 trl-0.9.4 tyro-0.8.5\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## HuggingFace SetUp","metadata":{}},{"cell_type":"code","source":"# from huggingface_hub import notebook_login\n# notebook_login()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\"","metadata":{"execution":{"iopub.execute_input":"2024-03-14T01:39:41.100571Z","iopub.status.busy":"2024-03-14T01:39:41.099815Z","iopub.status.idle":"2024-03-14T01:39:42.663452Z","shell.execute_reply":"2024-03-14T01:39:42.662351Z","shell.execute_reply.started":"2024-03-14T01:39:41.100539Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\nfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-07-02T14:17:42.851676Z","iopub.execute_input":"2024-07-02T14:17:42.852047Z","iopub.status.idle":"2024-07-02T14:18:01.358386Z","shell.execute_reply.started":"2024-07-02T14:17:42.852010Z","shell.execute_reply":"2024-07-02T14:18:01.357597Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-07-02 14:17:51.950357: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-02 14:17:51.950461: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-02 14:17:52.089801: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model SetUp","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\n\n# Log in to Hugging Face Hub\napi_token = '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'\nlogin(api_token)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T14:18:03.775044Z","iopub.execute_input":"2024-07-02T14:18:03.776192Z","iopub.status.idle":"2024-07-02T14:18:03.916612Z","shell.execute_reply.started":"2024-07-02T14:18:03.776158Z","shell.execute_reply":"2024-07-02T14:18:03.915763Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = \"meta-llama/Llama-2-7b-hf\"\n\ncompute_dtype = getattr(torch, \"float16\")\nbnb_config = BitsAndBytesConfig(load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=compute_dtype,\n            bnb_4bit_use_double_quant=True,\n)\nmodel = AutoModelForCausalLM.from_pretrained(model_name,quantization_config=bnb_config, device_map={\"\": 0})\nmodel = prepare_model_for_kbit_training(model)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:31:43.903555Z","iopub.execute_input":"2024-07-02T05:31:43.904150Z","iopub.status.idle":"2024-07-02T05:32:50.868128Z","shell.execute_reply.started":"2024-07-02T05:31:43.904120Z","shell.execute_reply":"2024-07-02T05:32:50.867030Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5644959ba4c6475fabaaa81d7a1e1c37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b8dd8a675b74fab9b490eb837d48224"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be0e362bfd35496fa9410fbd3516b155"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff5782c55b98494e803b357f6fbf046d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad3313094d8f46f38f3e26c1348b7a73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c3aabf8fd2e427a9954528e2a0c5c0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f7d4a16573b41bb9d770c158519b87b"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, add_eos_token=True)\ntokenizer.pad_token = tokenizer.unk_token\ntokenizer.padding_side = \"left\"\n# tokenizer.add_special_tokens({'pad_token': '[PAD]'})","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:33:17.434398Z","iopub.execute_input":"2024-07-02T05:33:17.435115Z","iopub.status.idle":"2024-07-02T05:33:18.956989Z","shell.execute_reply.started":"2024-07-02T05:33:17.435084Z","shell.execute_reply":"2024-07-02T05:33:18.955980Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77416da02427435786910c891cb5b2b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abcfb61eb66f42eb81c4482f034c5078"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb8d623035064b9ea6d71fb8c105f26a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7ccaff643fe4b2c9ca357c19777296b"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Load Model","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"musfiqdehan/preprocessed-BanglaNMT-sm\")","metadata":{"execution":{"iopub.status.busy":"2024-06-28T13:31:19.866978Z","iopub.execute_input":"2024-06-28T13:31:19.867356Z","iopub.status.idle":"2024-06-28T13:31:25.493000Z","shell.execute_reply.started":"2024-06-28T13:31:19.867325Z","shell.execute_reply":"2024-06-28T13:31:25.492114Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/27.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"416fdde85b4745d0b4d58eeafb533186"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/31.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0737b972e0454f14884a76af50a511e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/3.86M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4a729f503ec41ec9571e038c6d0730c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/3.85M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb85fa4563a24a2fb7d69e6b6cd54791"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/164084 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cede57cc357f4dd89986687d508891df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/20511 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef998326f754486c99bb9cfd6e96e58b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/20511 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0177557e3cac467fb03463abc145457b"}},"metadata":{}}]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-06-28T13:32:25.393932Z","iopub.execute_input":"2024-06-28T13:32:25.394821Z","iopub.status.idle":"2024-06-28T13:32:25.400356Z","shell.execute_reply.started":"2024-06-28T13:32:25.394788Z","shell.execute_reply":"2024-06-28T13:32:25.399317Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['translations'],\n        num_rows: 164084\n    })\n    validation: Dataset({\n        features: ['translations'],\n        num_rows: 20511\n    })\n    test: Dataset({\n        features: ['translations'],\n        num_rows: 20511\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset['train']['translations'][9]","metadata":{"execution":{"iopub.status.busy":"2024-06-28T13:32:05.818644Z","iopub.execute_input":"2024-06-28T13:32:05.819356Z","iopub.status.idle":"2024-06-28T13:32:06.208646Z","shell.execute_reply.started":"2024-06-28T13:32:05.819324Z","shell.execute_reply":"2024-06-28T13:32:06.207687Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'শারিব আলী নামের একজন তরুণদের প্লাটফর্ম পুলইজওয়ানএ আয়োজন সম্পর্কে বলেছেন ###>Sharib Ali on the youth platform PulEJawan described the event\\n'"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import DatasetDict, Dataset\nimport pyarrow as pa\n\n# Read the data from the file\nwith open('/kaggle/input/english-malayalam-valid-llama-fine/validation.txt', 'r', encoding='utf-8') as file:\n    translations = file.readlines()\n\n# Prepare data in the format expected by pyarrow\ndata = {\n    'translations': translations\n}\n\n# Create a pyarrow Table\narrow_table = pa.Table.from_pydict(data)\n\n# Create the Dataset object\ndataset = Dataset(arrow_table)\n\n# Create the DatasetDict object\ndataset_dict = DatasetDict({\n    'train': dataset\n})\n\n# Now 'dataset_dict' contains your data in the desired format\nprint(dataset_dict)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:48:38.506654Z","iopub.execute_input":"2024-06-29T08:48:38.507519Z","iopub.status.idle":"2024-06-29T08:48:38.591463Z","shell.execute_reply.started":"2024-06-29T08:48:38.507485Z","shell.execute_reply":"2024-06-29T08:48:38.590529Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['translations'],\n        num_rows: 4875\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import DatasetDict, Dataset\nimport pyarrow as pa\n\n# Read the data from the file\nwith open('/kaggle/input/test-llama-trans/test.txt', 'r', encoding='utf-8') as file:\n    translations = file.readlines()\n\n# Prepare data in the format expected by pyarrow\ndata = {\n    'translations': translations\n}\n\n# Create a pyarrow Table\narrow_table = pa.Table.from_pydict(data)\n\n# Create the Dataset object\ndataset = Dataset(arrow_table)\n\n# Create the DatasetDict object\ndataset_dict2 = DatasetDict({\n    'train': dataset\n})\n\n# Now 'dataset_dict' contains your data in the desired format\nprint(dataset_dict2)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-29T08:48:39.882748Z","iopub.execute_input":"2024-06-29T08:48:39.883119Z","iopub.status.idle":"2024-06-29T08:48:39.937281Z","shell.execute_reply.started":"2024-06-29T08:48:39.883093Z","shell.execute_reply":"2024-06-29T08:48:39.936316Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['translations'],\n        num_rows: 4903\n    })\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# LoRA Configuration","metadata":{}},{"cell_type":"code","source":"peft_config = LoraConfig(\n            lora_alpha=16, \n            lora_dropout=0.05,\n            r=16,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\",\n            target_modules= [\"down_proj\",\"up_proj\",\"gate_proj\"]\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:33:24.379224Z","iopub.execute_input":"2024-07-02T05:33:24.379588Z","iopub.status.idle":"2024-07-02T05:33:24.384721Z","shell.execute_reply.started":"2024-07-02T05:33:24.379561Z","shell.execute_reply":"2024-07-02T05:33:24.383640Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# peft_config = LoraConfig(\n#             lora_alpha=16,\n#             lora_dropout=0.05,\n#             r=64,\n#             bias=\"none\",\n#             task_type=\"CAUSAL_LM\",\n#             target_modules= [\"q_proj\",\"up_proj\",\"o_proj\",\"k_proj\",\"down_proj\",\"gate_proj\",\"v_proj\"]\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Hyperparameters","metadata":{}},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n        output_dir=\"working/results/\",\n        evaluation_strategy=\"steps\",\n        optim=\"paged_adamw_8bit\",\n        save_steps=100,\n        log_level=\"debug\",\n        logging_steps=100,\n        learning_rate=1e-4,\n        eval_steps=100,\n        fp16=True,\n        do_eval=True,\n        per_device_train_batch_size=48,\n        per_device_eval_batch_size=48,\n        gradient_accumulation_steps=2,\n        warmup_steps=50,\n        max_steps=500,\n        lr_scheduler_type=\"linear\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:33:27.818313Z","iopub.execute_input":"2024-07-02T05:33:27.819188Z","iopub.status.idle":"2024-07-02T05:33:27.853322Z","shell.execute_reply.started":"2024-07-02T05:33:27.819132Z","shell.execute_reply":"2024-07-02T05:33:27.852376Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training with TRL","metadata":{}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.execute_input":"2024-03-14T01:45:45.857270Z","iopub.status.busy":"2024-03-14T01:45:45.856339Z","iopub.status.idle":"2024-03-14T01:45:46.896707Z","shell.execute_reply":"2024-03-14T01:45:46.895448Z","shell.execute_reply.started":"2024-03-14T01:45:45.857229Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","output_type":"stream","text":"Thu Mar 14 01:45:46 2024       \n\n+---------------------------------------------------------------------------------------+\n\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n\n|-----------------------------------------+----------------------+----------------------+\n\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n\n|                                         |                      |               MIG M. |\n\n|=========================================+======================+======================|\n\n|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n\n| N/A   36C    P0              32W / 250W |   5362MiB / 16384MiB |      0%      Default |\n\n|                                         |                      |                  N/A |\n\n+-----------------------------------------+----------------------+----------------------+\n\n                                                                                         \n\n+---------------------------------------------------------------------------------------+\n\n| Processes:                                                                            |\n\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n\n|        ID   ID                                                             Usage      |\n\n|=======================================================================================|\n\n+---------------------------------------------------------------------------------------+\n"}]},{"cell_type":"code","source":"trainer = SFTTrainer(\n        model=model,\n        train_dataset=dataset_dict_shuffled['train'],\n        eval_dataset=dataset_dict_shuffled['validation'],\n        peft_config=peft_config,\n        dataset_text_field=\"translations\",\n        max_seq_length=48,\n        tokenizer=tokenizer,\n        args=training_arguments\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:33:33.262612Z","iopub.execute_input":"2024-07-02T05:33:33.262944Z","iopub.status.idle":"2024-07-02T09:33:49.981457Z","shell.execute_reply.started":"2024-07-02T05:33:33.262918Z","shell.execute_reply":"2024-07-02T09:33:49.980279Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1961: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1961: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/30000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"180835e4a3c44322b1aae1495127a1c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5eb75d20de7f416a8e7955b684e3433b"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:397: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n  warnings.warn(\nmax_steps is given, it will override any value given in num_train_epochs\nUsing auto half precision backend\nCurrently training with a batch size of: 48\n***** Running training *****\n  Num examples = 30,000\n  Num Epochs = 2\n  Instantaneous batch size per device = 48\n  Total train batch size (w. parallel, distributed & accumulation) = 96\n  Gradient Accumulation steps = 2\n  Total optimization steps = 500\n  Number of trainable parameters = 23,199,744\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240702_053423-x0irbfbv</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/pmabhinav20/huggingface/runs/x0irbfbv' target=\"_blank\">working/results/</a></strong> to <a href='https://wandb.ai/pmabhinav20/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/pmabhinav20/huggingface' target=\"_blank\">https://wandb.ai/pmabhinav20/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/pmabhinav20/huggingface/runs/x0irbfbv' target=\"_blank\">https://wandb.ai/pmabhinav20/huggingface/runs/x0irbfbv</a>"},"metadata":{}},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 3:58:41, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>2.325600</td>\n      <td>1.788400</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.703200</td>\n      <td>1.700193</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.653900</td>\n      <td>1.659043</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.592300</td>\n      <td>1.638666</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.576000</td>\n      <td>1.627747</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"\n***** Running Evaluation *****\n  Num examples = 2000\n  Batch size = 48\nWe detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\nSaving model checkpoint to working/results/checkpoint-100\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 11008,\n  \"max_position_embeddings\": 4096,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 32,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.42.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\ntokenizer config file saved in working/results/checkpoint-100/tokenizer_config.json\nSpecial tokens file saved in working/results/checkpoint-100/special_tokens_map.json\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n\n***** Running Evaluation *****\n  Num examples = 2000\n  Batch size = 48\nSaving model checkpoint to working/results/checkpoint-200\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 11008,\n  \"max_position_embeddings\": 4096,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 32,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.42.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\ntokenizer config file saved in working/results/checkpoint-200/tokenizer_config.json\nSpecial tokens file saved in working/results/checkpoint-200/special_tokens_map.json\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n\n***** Running Evaluation *****\n  Num examples = 2000\n  Batch size = 48\nSaving model checkpoint to working/results/checkpoint-300\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 11008,\n  \"max_position_embeddings\": 4096,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 32,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.42.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\ntokenizer config file saved in working/results/checkpoint-300/tokenizer_config.json\nSpecial tokens file saved in working/results/checkpoint-300/special_tokens_map.json\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n\n***** Running Evaluation *****\n  Num examples = 2000\n  Batch size = 48\nSaving model checkpoint to working/results/checkpoint-400\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 11008,\n  \"max_position_embeddings\": 4096,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 32,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.42.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\ntokenizer config file saved in working/results/checkpoint-400/tokenizer_config.json\nSpecial tokens file saved in working/results/checkpoint-400/special_tokens_map.json\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n\n***** Running Evaluation *****\n  Num examples = 2000\n  Batch size = 48\nSaving model checkpoint to working/results/checkpoint-500\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 11008,\n  \"max_position_embeddings\": 4096,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 32,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.42.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\ntokenizer config file saved in working/results/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in working/results/checkpoint-500/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=500, training_loss=1.7702031860351561, metrics={'train_runtime': 14411.0482, 'train_samples_per_second': 3.331, 'train_steps_per_second': 0.035, 'total_flos': 9.166063140864e+16, 'train_loss': 1.7702031860351561, 'epoch': 1.6})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Inference: Translate with Llama 2","metadata":{}},{"cell_type":"markdown","source":"## Base Model SetUp","metadata":{}},{"cell_type":"code","source":"base_model = \"meta-llama/Llama-2-7b-hf\"\ncompute_dtype = getattr(torch, \"float16\")\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=True,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n        base_model, device_map={\"\": 0}, quantization_config=bnb_config\n)\ntokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:36:44.869190Z","iopub.execute_input":"2024-07-02T09:36:44.869609Z","iopub.status.idle":"2024-07-02T09:36:52.867093Z","shell.execute_reply.started":"2024-07-02T09:36:44.869580Z","shell.execute_reply":"2024-07-02T09:36:52.865981Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 11008,\n  \"max_position_embeddings\": 4096,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 32,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.42.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/model.safetensors.index.json\nInstantiating LlamaForCausalLM model under default dtype torch.float16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c645ba9056914dd3a59ced366523a127"}},"metadata":{}},{"name":"stderr","text":"All model checkpoint weights were used when initializing LlamaForCausalLM.\n\nAll the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-hf.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"do_sample\": true,\n  \"eos_token_id\": 2,\n  \"max_length\": 4096,\n  \"pad_token_id\": 0,\n  \"temperature\": 0.6,\n  \"top_p\": 0.9\n}\n\nloading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.model\nloading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.json\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/special_tokens_map.json\nloading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer_config.json\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Initialize Adapter (Fine-Tuned-Model)","metadata":{}},{"cell_type":"code","source":"# Fetched from Kaggle Output\nmodel = PeftModel.from_pretrained(model, \"working/results/checkpoint-500/\")","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:37:14.715332Z","iopub.execute_input":"2024-07-02T09:37:14.715723Z","iopub.status.idle":"2024-07-02T09:37:15.258389Z","shell.execute_reply.started":"2024-07-02T09:37:14.715684Z","shell.execute_reply":"2024-07-02T09:37:15.257326Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Uploaded to Hugging Face Model Hub\n# model = PeftModel.from_pretrained(model, \"musfiqdehan/Llama-2-7b-ft-mt-Bengali-to-English-sm\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing Manually","metadata":{}},{"cell_type":"code","source":"my_text = \"Kerala, a state on India's tropical Malabar Coast\"\n\nprompt = my_text+\" #hi#>\"\n\ntokenized_input = tokenizer(prompt, return_tensors=\"pt\")\ninput_ids = tokenized_input[\"input_ids\"].cuda()\n\ngeneration_output = model.generate(\n        input_ids=input_ids,\n        num_beams=6,\n        return_dict_in_generate=True,\n        output_scores=True,\n        max_new_tokens=130\n)\nfor seq in generation_output.sequences:\n    output = tokenizer.decode(seq, skip_special_tokens=True)\n    print(output.split(\"#hi#>\")[1].strip()) ","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:38:33.916591Z","iopub.execute_input":"2024-07-02T09:38:33.916943Z","iopub.status.idle":"2024-07-02T09:38:41.487345Z","shell.execute_reply.started":"2024-07-02T09:38:33.916917Z","shell.execute_reply":"2024-07-02T09:38:41.486227Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"भारत की तटीय मलबार कोष्ठ\n","output_type":"stream"}]},{"cell_type":"code","source":"my_text = \"Kerala, a state on India's tropical Malabar Coast\"\n\nprompt = my_text+\" #ml#>\"\n\ntokenized_input = tokenizer(prompt, return_tensors=\"pt\")\ninput_ids = tokenized_input[\"input_ids\"].cuda()\n\ngeneration_output = model.generate(\n        input_ids=input_ids,\n        num_beams=6,\n        return_dict_in_generate=True,\n        output_scores=True,\n        max_new_tokens=130\n)\nfor seq in generation_output.sequences:\n    output = tokenizer.decode(seq, skip_special_tokens=True)\n    print(output.split(\"#ml#>\")[1].strip()) ","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:38:44.107292Z","iopub.execute_input":"2024-07-02T09:38:44.107895Z","iopub.status.idle":"2024-07-02T09:38:51.423632Z","shell.execute_reply.started":"2024-07-02T09:38:44.107859Z","shell.execute_reply":"2024-07-02T09:38:51.422499Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"ഇന്ത്യയിലെ മലബാർ കോ\n","output_type":"stream"}]},{"cell_type":"code","source":"def translator(text, language_code):\n    prompt = text + \" \" + language_code\n    tokenized_input = tokenizer(prompt, return_tensors=\"pt\")\n    input_ids = tokenized_input[\"input_ids\"].cuda()\n\n    generation_output = model.generate(\n        input_ids=input_ids,\n        num_beams=6,\n        return_dict_in_generate=True,\n        output_scores=True,\n        max_new_tokens=130\n    )\n    for seq in generation_output.sequences:\n        output = tokenizer.decode(seq, skip_special_tokens=True)\n        return output.split(language_code)[1].strip()\n\n# Example usage\nmy_text = \"Kerala, a state on India's tropical Malabar Coast\"\ntranslated_text_ml = translator(my_text, \"#ml#>\")\ntranslated_text_hi = translator(my_text, \"#hi#>\")","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:56:06.927277Z","iopub.execute_input":"2024-07-02T09:56:06.928178Z","iopub.status.idle":"2024-07-02T09:56:21.791240Z","shell.execute_reply.started":"2024-07-02T09:56:06.928135Z","shell.execute_reply":"2024-07-02T09:56:21.790147Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"translated_text_hi","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:56:59.173239Z","iopub.execute_input":"2024-07-02T09:56:59.173613Z","iopub.status.idle":"2024-07-02T09:56:59.181232Z","shell.execute_reply.started":"2024-07-02T09:56:59.173584Z","shell.execute_reply":"2024-07-02T09:56:59.180287Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"'भारत की तटीय मलबार कोष्ठ'"},"metadata":{}}]},{"cell_type":"code","source":"translated_text_ml","metadata":{"execution":{"iopub.status.busy":"2024-07-02T09:56:54.575240Z","iopub.execute_input":"2024-07-02T09:56:54.575903Z","iopub.status.idle":"2024-07-02T09:56:54.583000Z","shell.execute_reply.started":"2024-07-02T09:56:54.575874Z","shell.execute_reply":"2024-07-02T09:56:54.581947Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"'ഇന്ത്യയിലെ മലബാർ കോ'"},"metadata":{}}]},{"cell_type":"code","source":"def translate_texts(translator, dataset, language_code):\n    tgt_texts, trans_texts = [], []\n\n    for translation in dataset['translations']:\n        src_text, tgt_text = translation.split(language_code)\n        translated_text = translator(src_text.strip(), language_code)\n        tgt_texts.append(tgt_text.strip())\n        trans_texts.append(translated_text)\n\n    return tgt_texts, trans_texts","metadata":{"execution":{"iopub.status.busy":"2024-07-02T10:03:21.537227Z","iopub.execute_input":"2024-07-02T10:03:21.537632Z","iopub.status.idle":"2024-07-02T10:03:21.545557Z","shell.execute_reply.started":"2024-07-02T10:03:21.537599Z","shell.execute_reply":"2024-07-02T10:03:21.544605Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"!pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2024-07-02T10:04:07.062517Z","iopub.execute_input":"2024-07-02T10:04:07.063341Z","iopub.status.idle":"2024-07-02T10:04:20.087878Z","shell.execute_reply.started":"2024-07-02T10:04:07.063303Z","shell.execute_reply":"2024-07-02T10:04:20.086604Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.20.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.1)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.3.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.23.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.2\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install sacrebleu","metadata":{"execution":{"iopub.status.busy":"2024-07-02T11:40:43.035280Z","iopub.execute_input":"2024-07-02T11:40:43.036002Z","iopub.status.idle":"2024-07-02T11:40:56.340269Z","shell.execute_reply.started":"2024-07-02T11:40:43.035967Z","shell.execute_reply":"2024-07-02T11:40:56.338993Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting sacrebleu\n  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-2.10.0-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2023.12.25)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.2.2)\nDownloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-2.10.0-py3-none-any.whl (18 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-2.10.0 sacrebleu-2.4.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-07-02T10:04:44.675534Z","iopub.execute_input":"2024-07-02T10:04:44.675916Z","iopub.status.idle":"2024-07-02T10:04:45.040803Z","shell.execute_reply.started":"2024-07-02T10:04:44.675887Z","shell.execute_reply":"2024-07-02T10:04:45.039816Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"def postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n    return preds, labels\n\n# Define compute_metrics function\ndef compute_metrics(decoded_preds, decoded_labels):\n    # Load evaluation metric\n    metric = evaluate.load(\"sacrebleu\")\n    # Post-process the decoded predictions and labels\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n    # Compute BLEU score\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n    # Compile results\n    result = {'bleu': result['score']}\n    result = {k: round(v, 4) for k, v in result.items()}\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-07-02T11:47:21.745869Z","iopub.execute_input":"2024-07-02T11:47:21.746296Z","iopub.status.idle":"2024-07-02T11:47:21.755545Z","shell.execute_reply.started":"2024-07-02T11:47:21.746264Z","shell.execute_reply":"2024-07-02T11:47:21.754537Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"metrics = compute_metrics(tgt_texts, trans_texts)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T11:47:31.398094Z","iopub.execute_input":"2024-07-02T11:47:31.398924Z","iopub.status.idle":"2024-07-02T11:47:32.113079Z","shell.execute_reply.started":"2024-07-02T11:47:31.398889Z","shell.execute_reply":"2024-07-02T11:47:32.112019Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"metrics","metadata":{"execution":{"iopub.status.busy":"2024-07-02T11:47:35.708328Z","iopub.execute_input":"2024-07-02T11:47:35.709065Z","iopub.status.idle":"2024-07-02T11:47:35.716500Z","shell.execute_reply.started":"2024-07-02T11:47:35.709031Z","shell.execute_reply":"2024-07-02T11:47:35.715525Z"},"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"{'bleu': 1.3788}"},"metadata":{}}]},{"cell_type":"code","source":"def postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n    return preds, labels\n\n# Define compute_metrics function\ndef compute_metrics(decoded_preds, decoded_labels):\n    # Load evaluation metrics\n    sacrebleu_metric = evaluate.load(\"sacrebleu\")\n    chrf_metric = evaluate.load(\"chrf\")\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    # Compute BLEU score\n    sacrebleu_result = sacrebleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n\n    # Compute CHRF score\n    chrf_result = chrf_metric.compute(predictions=decoded_preds, references=decoded_labels)\n\n    result = {\n        'bleu': sacrebleu_result['score'],\n        'chrf': chrf_result['score']\n    }\n    result = {k: round(v, 4) for k, v in result.items()}\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-07-02T13:38:04.333189Z","iopub.execute_input":"2024-07-02T13:38:04.333573Z","iopub.status.idle":"2024-07-02T13:38:04.343362Z","shell.execute_reply.started":"2024-07-02T13:38:04.333543Z","shell.execute_reply":"2024-07-02T13:38:04.342374Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport sacrebleu\nimport evaluate\nfrom datasets import DatasetDict\n\n# Load the BLEU metric from the evaluate library\nmetric = evaluate.load(\"bleu\")\n\n# Define the postprocess_text function\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [label.strip() for label in labels]\n    return preds, labels\n\n# Define the compute_metrics function\ndef compute_metrics(tgt_texts, trans_texts):\n    # Post-process the target and translated texts\n    decoded_preds, decoded_labels = postprocess_text(trans_texts, tgt_texts)\n    # Compute BLEU score using the metric\n    result = metric.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])\n    # Create the result dictionary\n    result = {'bleu': result['bleu']}\n    # Round the results for readability\n    result = {k: round(v, 4) for k, v in result.items()}\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-07-02T11:45:46.706911Z","iopub.execute_input":"2024-07-02T11:45:46.707943Z","iopub.status.idle":"2024-07-02T11:45:47.427862Z","shell.execute_reply.started":"2024-07-02T11:45:46.707905Z","shell.execute_reply":"2024-07-02T11:45:47.426764Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"metrics = compute_metrics(tgt_texts, trans_texts)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T11:46:11.222546Z","iopub.execute_input":"2024-07-02T11:46:11.223283Z","iopub.status.idle":"2024-07-02T11:46:11.494644Z","shell.execute_reply.started":"2024-07-02T11:46:11.223250Z","shell.execute_reply":"2024-07-02T11:46:11.493532Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"metrics","metadata":{"execution":{"iopub.status.busy":"2024-07-02T11:46:13.446916Z","iopub.execute_input":"2024-07-02T11:46:13.447632Z","iopub.status.idle":"2024-07-02T11:46:13.455082Z","shell.execute_reply.started":"2024-07-02T11:46:13.447597Z","shell.execute_reply":"2024-07-02T11:46:13.453871Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"{'bleu': 0.0003}"},"metadata":{}}]},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\n\ndef bleu_score2(tgt_texts, trans_texts):\n    # Prepare the target and translated texts for BLEU calculation\n    tgt_texts2 = [[tgt_text.split()] for tgt_text in tgt_texts]\n    trans_text2 = [translated_text.split() for translated_text in trans_texts]\n\n    # Calculate BLEU scores for different n-gram weights\n    bleu_dic = {}\n    bleu_dic['1-grams'] = corpus_bleu(tgt_texts2, trans_text2, weights=(1.0, 0, 0, 0))\n    bleu_dic['1-2-grams'] = corpus_bleu(tgt_texts2, trans_text2, weights=(0.5, 0.5, 0, 0))\n    bleu_dic['1-3-grams'] = corpus_bleu(tgt_texts2, trans_text2, weights=(0.3, 0.3, 0.3, 0))\n    bleu_dic['1-4-grams'] = corpus_bleu(tgt_texts2, trans_text2, weights=(0.25, 0.25, 0.25, 0.25))\n\n    # Calculate the average BLEU score\n    average_bleu = sum(bleu_dic.values()) / len(bleu_dic)\n    bleu_dic['average'] = average_bleu\n\n    return bleu_dic\n\ndef bleu_score3(tgt_texts, trans_texts):\n    # Prepare the target and translated texts for BLEU calculation\n    tgt_texts2 = [[tgt_text.split()] for tgt_text in tgt_texts]\n    trans_text2 = [translated_text.split() for translated_text in trans_texts]\n\n    # Calculate BLEU scores for different n-gram weights\n    bleu_dic = {}\n    bleu_dic['BLEU-1'] = corpus_bleu(tgt_texts2, trans_text2, weights=(1.0, 0, 0, 0))\n    bleu_dic['BLEU-2'] = corpus_bleu(tgt_texts2, trans_text2, weights=(0.5, 0.5, 0, 0))\n    bleu_dic['BLEU-3'] = corpus_bleu(tgt_texts2, trans_text2, weights=(0.33, 0.33, 0.34, 0))\n    bleu_dic['BLEU-4'] = corpus_bleu(tgt_texts2, trans_text2, weights=(0.25, 0.25, 0.25, 0.25))\n\n    # Calculate the average BLEU score as per the paper's formula\n    average_bleu = (bleu_dic['BLEU-1'] + bleu_dic['BLEU-2'] + bleu_dic['BLEU-3'] + bleu_dic['BLEU-4']) / 4\n    bleu_dic['BLEU_Avg'] = average_bleu * 100  # Multiplying by 100 to match the paper's scaling\n\n    return bleu_dic\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T11:44:03.768877Z","iopub.execute_input":"2024-07-02T11:44:03.769265Z","iopub.status.idle":"2024-07-02T11:44:04.824422Z","shell.execute_reply.started":"2024-07-02T11:44:03.769235Z","shell.execute_reply":"2024-07-02T11:44:04.823381Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"bleu_scores2 = bleu_score2(tgt_texts, trans_texts)\nbleu_scores3 = bleu_score3(tgt_texts, trans_texts)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T11:44:41.105367Z","iopub.execute_input":"2024-07-02T11:44:41.106207Z","iopub.status.idle":"2024-07-02T11:44:41.807456Z","shell.execute_reply.started":"2024-07-02T11:44:41.106173Z","shell.execute_reply":"2024-07-02T11:44:41.806416Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"bleu_scores2","metadata":{"execution":{"iopub.status.busy":"2024-07-02T11:44:48.782791Z","iopub.execute_input":"2024-07-02T11:44:48.783485Z","iopub.status.idle":"2024-07-02T11:44:48.790912Z","shell.execute_reply.started":"2024-07-02T11:44:48.783450Z","shell.execute_reply":"2024-07-02T11:44:48.789732Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"{'1-grams': 0.0008077470718690841,\n '1-2-grams': 0.000541608227878747,\n '1-3-grams': 0.00041137500109731333,\n '1-4-grams': 0.0002060647376368793,\n 'average': 0.000491698759620506}"},"metadata":{}}]},{"cell_type":"code","source":"bleu_scores3","metadata":{"execution":{"iopub.status.busy":"2024-07-02T11:44:51.770247Z","iopub.execute_input":"2024-07-02T11:44:51.770968Z","iopub.status.idle":"2024-07-02T11:44:51.777781Z","shell.execute_reply.started":"2024-07-02T11:44:51.770939Z","shell.execute_reply":"2024-07-02T11:44:51.776809Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"{'BLEU-1': 0.0008077470718690841,\n 'BLEU-2': 0.000541608227878747,\n 'BLEU-3': 0.0003462997304286147,\n 'BLEU-4': 0.0002060647376368793,\n 'BLEU_Avg': 0.04754299419533313}"},"metadata":{}}]},{"cell_type":"code","source":"tgt_texts, trans_texts = translate_texts(translator, dataset_dict_hi_test['test_hi'], \"#hi#>\")","metadata":{"execution":{"iopub.status.busy":"2024-07-02T10:05:06.043891Z","iopub.execute_input":"2024-07-02T10:05:06.044279Z","iopub.status.idle":"2024-07-02T11:27:07.784889Z","shell.execute_reply.started":"2024-07-02T10:05:06.044249Z","shell.execute_reply":"2024-07-02T11:27:07.783839Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"metrics = compute_metrics(tgt_texts, trans_texts)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T11:41:06.190266Z","iopub.execute_input":"2024-07-02T11:41:06.190623Z","iopub.status.idle":"2024-07-02T11:41:07.931113Z","shell.execute_reply.started":"2024-07-02T11:41:06.190597Z","shell.execute_reply":"2024-07-02T11:41:07.929846Z"},"trusted":true},"execution_count":48,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/9.01k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ca55447ae1848b6bab4b558be3cf79d"}},"metadata":{}}]},{"cell_type":"code","source":"metrics","metadata":{"execution":{"iopub.status.busy":"2024-07-02T11:41:14.649845Z","iopub.execute_input":"2024-07-02T11:41:14.650249Z","iopub.status.idle":"2024-07-02T11:41:14.658482Z","shell.execute_reply.started":"2024-07-02T11:41:14.650216Z","shell.execute_reply":"2024-07-02T11:41:14.657320Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"{'bleu': 1.3788, 'chrf': 23.6405}"},"metadata":{}}]},{"cell_type":"code","source":"tgt_textsML, trans_textsML = translate_texts(translator, dataset_dict_ml_test['test_ml'], \"#ml#>\")","metadata":{"execution":{"iopub.status.busy":"2024-07-02T11:48:10.596376Z","iopub.execute_input":"2024-07-02T11:48:10.596778Z","iopub.status.idle":"2024-07-02T13:21:09.409201Z","shell.execute_reply.started":"2024-07-02T11:48:10.596729Z","shell.execute_reply":"2024-07-02T13:21:09.408087Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"metrics = compute_metrics(tgt_textsML, trans_textsML)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T13:38:23.227709Z","iopub.execute_input":"2024-07-02T13:38:23.228081Z","iopub.status.idle":"2024-07-02T13:38:24.810585Z","shell.execute_reply.started":"2024-07-02T13:38:23.228050Z","shell.execute_reply":"2024-07-02T13:38:24.809500Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"metrics","metadata":{"execution":{"iopub.status.busy":"2024-07-02T13:38:30.027384Z","iopub.execute_input":"2024-07-02T13:38:30.027751Z","iopub.status.idle":"2024-07-02T13:38:30.034972Z","shell.execute_reply.started":"2024-07-02T13:38:30.027723Z","shell.execute_reply":"2024-07-02T13:38:30.034038Z"},"trusted":true},"execution_count":66,"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"{'bleu': 0.2663, 'chrf': 20.0673}"},"metadata":{}}]},{"cell_type":"code","source":"import sacrebleu\nimport evaluate\n\n# Load evaluation metrics\nsacrebleu_metric = evaluate.load(\"sacrebleu\")\nchrf_metric = evaluate.load(\"chrf\")\nter_metric = sacrebleu.metrics.TER()\n\n# Define the postprocess_text function\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [label.strip() for label in labels]\n    return preds, labels\n\n# Define the compute_metrics function\ndef compute_metrics(decoded_preds, decoded_labels):\n    # Post-process the decoded predictions and labels\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    # Compute BLEU score\n    sacrebleu_result = sacrebleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n    bleu_score = sacrebleu_result['score']\n\n    # Compute CHRF score\n    chrf_result = chrf_metric.compute(predictions=decoded_preds, references=decoded_labels)\n    chrf_score = chrf_result['score']\n\n    # Compute TER score using sacrebleu\n    ter_score = ter_metric.corpus_score(decoded_preds, [decoded_labels]).score\n\n    # Create the result dictionary\n    result = {\n        'bleu': round(bleu_score, 4),\n        'chrf': round(chrf_score, 4),\n        'ter': round(ter_score, 4)\n    }\n    \n    return result","metadata":{"execution":{"iopub.status.busy":"2024-07-02T13:42:42.173192Z","iopub.execute_input":"2024-07-02T13:42:42.173581Z","iopub.status.idle":"2024-07-02T13:42:43.173108Z","shell.execute_reply.started":"2024-07-02T13:42:42.173550Z","shell.execute_reply":"2024-07-02T13:42:43.172130Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"metrics = compute_metrics(tgt_texts, trans_texts)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T13:43:20.025628Z","iopub.execute_input":"2024-07-02T13:43:20.026233Z","iopub.status.idle":"2024-07-02T13:43:20.862081Z","shell.execute_reply.started":"2024-07-02T13:43:20.026202Z","shell.execute_reply":"2024-07-02T13:43:20.861036Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"metrics","metadata":{"execution":{"iopub.status.busy":"2024-07-02T13:43:21.667725Z","iopub.execute_input":"2024-07-02T13:43:21.668800Z","iopub.status.idle":"2024-07-02T13:43:21.678323Z","shell.execute_reply.started":"2024-07-02T13:43:21.668754Z","shell.execute_reply":"2024-07-02T13:43:21.677320Z"},"trusted":true},"execution_count":71,"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"{'bleu': 1.3788, 'chrf': 23.6405, 'ter': 688.4011}"},"metadata":{}}]},{"cell_type":"code","source":"metrics = compute_metrics(tgt_textsML, trans_textsML)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T13:42:57.574847Z","iopub.execute_input":"2024-07-02T13:42:57.575550Z","iopub.status.idle":"2024-07-02T13:42:58.240672Z","shell.execute_reply.started":"2024-07-02T13:42:57.575518Z","shell.execute_reply":"2024-07-02T13:42:58.239621Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"metrics","metadata":{"execution":{"iopub.status.busy":"2024-07-02T13:43:00.308412Z","iopub.execute_input":"2024-07-02T13:43:00.309254Z","iopub.status.idle":"2024-07-02T13:43:00.317686Z","shell.execute_reply.started":"2024-07-02T13:43:00.309213Z","shell.execute_reply":"2024-07-02T13:43:00.316339Z"},"trusted":true},"execution_count":69,"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"{'bleu': 0.2663, 'chrf': 20.0673, 'ter': 502.6419}"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\n# Assuming you have your model and tokenizer already loaded and fine-tuned\nmodel_name = \"meta-llama/Llama-2-7b-hf\"\nbase_model = \"meta-llama/Llama-2-7b-hf\"\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=True,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model, device_map={\"\": 0}, quantization_config=bnb_config\n)\ntokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n\n# Load the fine-tuned model\nmodel = PeftModel.from_pretrained(model, \"working/results/checkpoint-500/\")\n\n# Push the model to Hugging Face Hub\nmodel.push_to_hub(\"FineTuned-Trans-oneTomany-llama-2-7b\")\ntokenizer.push_to_hub(\"FineTuned-Trans-oneTomany-llama-2-7b\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T13:53:49.345465Z","iopub.execute_input":"2024-07-02T13:53:49.345865Z","iopub.status.idle":"2024-07-02T13:54:03.504352Z","shell.execute_reply.started":"2024-07-02T13:53:49.345836Z","shell.execute_reply":"2024-07-02T13:54:03.503124Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 11008,\n  \"max_position_embeddings\": 4096,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 32,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.42.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/model.safetensors.index.json\nInstantiating LlamaForCausalLM model under default dtype torch.float16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0847dbea1cf6448abbe907e4a2d20b0b"}},"metadata":{}},{"name":"stderr","text":"All model checkpoint weights were used when initializing LlamaForCausalLM.\n\nAll the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-hf.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"do_sample\": true,\n  \"eos_token_id\": 2,\n  \"max_length\": 4096,\n  \"pad_token_id\": 0,\n  \"temperature\": 0.6,\n  \"top_p\": 0.9\n}\n\nloading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.model\nloading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.json\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/special_tokens_map.json\nloading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer_config.json\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 11008,\n  \"max_position_embeddings\": 4096,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 32,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.42.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nUploading the following files to ABHIiiii1/FineTuned-Trans-oneTomany-llama-2-7b: adapter_config.json,README.md,adapter_model.safetensors\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/92.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3c4a4350f324886a8abea949348fa0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5896a1b50c224e53afd9aadf85d848e4"}},"metadata":{}},{"name":"stderr","text":"tokenizer config file saved in /tmp/tmp65moef8m/tokenizer_config.json\nSpecial tokens file saved in /tmp/tmp65moef8m/special_tokens_map.json\nUploading the following files to ABHIiiii1/FineTuned-Trans-oneTomany-llama-2-7b: special_tokens_map.json,tokenizer.json,tokenizer_config.json,tokenizer.model,README.md\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de9b608164c44370a2e74fbb131e301a"}},"metadata":{}},{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/ABHIiiii1/FineTuned-Trans-oneTomany-llama-2-7b/commit/5e4d3d208228f25924352f6a9de5cfdbd25cd12b', commit_message='Upload tokenizer', commit_description='', oid='5e4d3d208228f25924352f6a9de5cfdbd25cd12b', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"import torch","metadata":{"execution":{"iopub.status.busy":"2024-07-02T14:15:14.218381Z","iopub.execute_input":"2024-07-02T14:15:14.219074Z","iopub.status.idle":"2024-07-02T14:15:14.223596Z","shell.execute_reply.started":"2024-07-02T14:15:14.219042Z","shell.execute_reply":"2024-07-02T14:15:14.222428Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load the model and tokenizer from the Hugging Face Hub\nmodel_name = \"ABHIiiii1/FineTuned-Trans-oneTomany-llama-2-7b\"  # Replace with your model's path\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Move the model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Set the model to evaluation mode\nmodel.eval()\n\ndef translator(text, language_code):\n    prompt = text + \" \" + language_code\n    tokenized_input = tokenizer(prompt, return_tensors=\"pt\")\n    input_ids = tokenized_input[\"input_ids\"].to(device)\n\n    generation_output = model.generate(\n        input_ids=input_ids,\n        num_beams=6,\n        return_dict_in_generate=True,\n        output_scores=True,\n        max_new_tokens=130\n    )\n    for seq in generation_output.sequences:\n        output = tokenizer.decode(seq, skip_special_tokens=True)\n        return output.split(language_code)[1].strip()\n\n# Example usage\nmy_text = \"Kerala, a state on India's tropical Malabar Coast\"\ntranslated_text_ml = translator(my_text, \"#ml#>\")\ntranslated_text_hi = translator(my_text, \"#hi#>\")","metadata":{"execution":{"iopub.status.busy":"2024-07-02T14:18:16.361903Z","iopub.execute_input":"2024-07-02T14:18:16.362648Z","iopub.status.idle":"2024-07-02T14:20:09.344923Z","shell.execute_reply.started":"2024-07-02T14:18:16.362599Z","shell.execute_reply":"2024-07-02T14:20:09.343593Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/946 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a91f0bc402d4e49917ee7b5e8ba1938"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a4f33f38dd148c28563dc0065366564"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14d3c31d7bec4f428b7dd6ec471e85fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17e33516ecce4ba89dd0e6f29f09177b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/671 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"033d1e1043e14f3798105706d82f0722"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca986a75a013484b93849ddb51cae6df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c94f9818080e41678a542b819ee79b41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ac7f4cad67e44df8550b19a9e4eb79c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56368f1b59c64d1ea5b79c1115ed9559"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfa0e02b301a4ef8bf188d59f9953335"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45c9a2fabb4d4351984d969f520cc1c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f14b8eed28414a318afad1ce1c823eea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/92.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1531fe12966e4e3e8bd376cf745d9c6a"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Move the model to GPU if available\u001b[39;00m\n\u001b[1;32m      9\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2796\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2792\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2793\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2794\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2795\u001b[0m         )\n\u001b[0;32m-> 2796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (2 times)]\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 20.12 MiB is free. Process 2159 has 15.87 GiB memory in use. Of the allocated memory 15.62 GiB is allocated by PyTorch, and 4.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 20.12 MiB is free. Process 2159 has 15.87 GiB memory in use. Of the allocated memory 15.62 GiB is allocated by PyTorch, and 4.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nimport torch\n\n# Load the model and tokenizer from the Hugging Face Hub\nmodel_name = \"ABHIiiii1/FineTuned-Trans-llama-2-7b\"  # Replace with your model's path\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = PeftModel.from_pretrained(AutoModelForCausalLM.from_pretrained(model_name), model_name)\n\n# Move the model to GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(\"cuda\")\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# Prepare the input text\nmy_text = \"Kerala, a state on India's tropical Malabar Coast\"\nprompt = my_text + \" ###>\"\n\n# Tokenize the input text\ntokenized_input = tokenizer(prompt, return_tensors=\"pt\")\ninput_ids = tokenized_input[\"input_ids\"].to(device)\n\n# Generate the translation\ngeneration_output = model.generate(\n    input_ids=input_ids,\n    num_beams=6,\n    return_dict_in_generate=True,\n    output_scores=True,\n    max_new_tokens=130\n)\n\n# Decode and print the generated text\nfor seq in generation_output.sequences:\n    output = tokenizer.decode(seq, skip_special_tokens=True)\n    print(output.split(\"###>\")[1].strip())\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:47:59.666212Z","iopub.execute_input":"2024-07-01T14:47:59.666899Z","iopub.status.idle":"2024-07-01T14:49:53.548685Z","shell.execute_reply.started":"2024-07-01T14:47:59.666864Z","shell.execute_reply":"2024-07-01T14:49:53.546629Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61d5c44222304f59b3eeb97db7c5a7d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f10e92555641454fa5ed319ee1a0a4da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1be052e7a22245df8b83d40c95597040"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e0f9813b2474674be449f7f045486da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"746e44297c13480499c8b2ffa46ce307"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25594b0da6f148c08634093e46bf83c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6ead14e888446acbefe78a9904d88ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/92.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"247239440f714496a4354754d497d271"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Move the model to GPU\u001b[39;00m\n\u001b[1;32m     11\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (5 times)]\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 192.12 MiB is free. Process 3238 has 15.71 GiB memory in use. Of the allocated memory 15.45 GiB is allocated by PyTorch, and 5.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 192.12 MiB is free. Process 3238 has 15.71 GiB memory in use. Of the allocated memory 15.45 GiB is allocated by PyTorch, and 5.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}