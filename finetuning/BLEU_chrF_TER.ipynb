{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U datasets transformers trl accelerate peft bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_eng_latn_mal_mlym = \"/kaggle/input/wiki-datatrans/wiki/eng_Latn-mal_Mlym/train.eng_Latn\"\n",
    "file_mal_mlym = \"/kaggle/input/wiki-datatrans/wiki/eng_Latn-mal_Mlym/train.mal_Mlym\"\n",
    "\n",
    "with open(file_eng_latn_mal_mlym, 'r', encoding='utf-8') as f_eng_latn, open(file_mal_mlym, 'r', encoding='utf-8') as f_mal_mlym:\n",
    "    eng_latn_mal_mlym_data = f_eng_latn.readlines()\n",
    "    mal_mlym_data = f_mal_mlym.readlines()\n",
    "\n",
    "# Combine into one dataset\n",
    "eng_latn_mal_mlym_pairs = list(zip(eng_latn_mal_mlym_data, mal_mlym_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_eng_latn_tam_taml = \"/kaggle/input/wiki-datatrans/wiki/eng_Latn-hin_Deva/train.eng_Latn\"\n",
    "file_tam_taml = \"/kaggle/input/wiki-datatrans/wiki/eng_Latn-hin_Deva/train.hin_Deva\"\n",
    "\n",
    "with open(file_eng_latn_tam_taml, 'r', encoding='utf-8') as f_eng_latn_tam_taml, open(file_tam_taml, 'r', encoding='utf-8') as f_tam_taml:\n",
    "    eng_latn_tam_taml_data = f_eng_latn_tam_taml.readlines()\n",
    "    tam_taml_data = f_tam_taml.readlines()\n",
    "\n",
    "# Combine into one dataset\n",
    "eng_latn_tam_taml_pairs = list(zip(eng_latn_tam_taml_data, tam_taml_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format English to Malayalam dataset\n",
    "formatted_eng_mal_dataset = [\n",
    "    f\"{eng.strip()} #ml#> {mal.strip()}\" for eng, mal in eng_latn_mal_mlym_pairs\n",
    "]\n",
    "\n",
    "# Format English to Hindi dataset\n",
    "formatted_eng_hi_dataset = [\n",
    "    f\"{eng.strip()} #hi#> {tam.strip()}\" for eng, tam in eng_latn_tam_taml_pairs\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Combine the datasets\n",
    "combined_dataset = formatted_eng_mal_dataset + formatted_eng_hi_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# # Shuffle the combined dataset\n",
    "# random.shuffle(combined_dataset)\n",
    "\n",
    "# Define the number of examples for each split\n",
    "train_size = 30000\n",
    "test_size = 2000\n",
    "validation_size = 2000\n",
    "\n",
    "# Initialize counters for each category (#ml#> and #hi#>)\n",
    "ml_count_train = 0\n",
    "hi_count_train = 0\n",
    "ml_count_test = 0\n",
    "hi_count_test = 0\n",
    "ml_count_val = 0\n",
    "hi_count_val = 0\n",
    "\n",
    "# Initialize lists for train, test, and validation datasets\n",
    "train_dataset = []\n",
    "test_dataset = []\n",
    "validation_dataset = []\n",
    "\n",
    "# Iterate through the combined dataset\n",
    "for pair in combined_dataset:\n",
    "    if '#ml#>' in pair:\n",
    "        if ml_count_train < train_size / 2:\n",
    "            train_dataset.append(pair)\n",
    "            ml_count_train += 1\n",
    "        elif ml_count_test < test_size / 2:\n",
    "            test_dataset.append(pair)\n",
    "            ml_count_test += 1\n",
    "        elif ml_count_val < validation_size / 2:\n",
    "            validation_dataset.append(pair)\n",
    "            ml_count_val += 1\n",
    "    elif '#hi#>' in pair:\n",
    "        if hi_count_train < train_size / 2:\n",
    "            train_dataset.append(pair)\n",
    "            hi_count_train += 1\n",
    "        elif hi_count_test < test_size / 2:\n",
    "            test_dataset.append(pair)\n",
    "            hi_count_test += 1\n",
    "        elif hi_count_val < validation_size / 2:\n",
    "            validation_dataset.append(pair)\n",
    "            hi_count_val += 1\n",
    "\n",
    "# Verify the sizes of each dataset\n",
    "print(f\"Train Dataset Size: {len(train_dataset)}\")\n",
    "print(f\"Test Dataset Size: {len(test_dataset)}\")\n",
    "print(f\"Validation Dataset Size: {len(validation_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Combine the datasets into a single list of dictionaries\n",
    "combined_data = {\n",
    "    \"translations\": train_dataset + validation_dataset + test_dataset\n",
    "}\n",
    "\n",
    "# Define lengths for each split\n",
    "train_length = len(train_dataset)\n",
    "validation_length = len(validation_dataset)\n",
    "test_length = len(test_dataset)\n",
    "\n",
    "# Create DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": Dataset.from_dict(combined_data).select(range(train_length)),\n",
    "    \"validation\": Dataset.from_dict(combined_data).select(range(train_length, train_length + validation_length)),\n",
    "    \"test\": Dataset.from_dict(combined_data).select(range(train_length + validation_length, train_length + validation_length + test_length)),\n",
    "})\n",
    "\n",
    "# Print the structure and sizes of the DatasetDict\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict['validation']['translations'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict['validation']['translations'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict_shuffled = dataset_dict.shuffle(seed=42)\n",
    "\n",
    "print(dataset_dict_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Assume dataset_dict_shuffled is already defined and shuffled\n",
    "\n",
    "# Filter out English to Hindi pairs (#hi#>) from the 'test' split\n",
    "def filter_hi(example):\n",
    "    return '#hi#>' in example['translations']\n",
    "\n",
    "# Apply the filter to the 'test' split\n",
    "test_hi_pairs = dataset_dict_shuffled['test'].filter(filter_hi)\n",
    "\n",
    "# Create a new DatasetDict for the filtered pairs\n",
    "dataset_dict_hi_test = DatasetDict({\n",
    "    \"test_hi\": test_hi_pairs\n",
    "})\n",
    "\n",
    "# Print the structure and size of the new DatasetDict\n",
    "print(dataset_dict_hi_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict_hi_test['test_hi']['translations'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Assume dataset_dict_shuffled is already defined and shuffled\n",
    "\n",
    "# Filter out English to Malayalam pairs (#ml#>) from the 'test' split\n",
    "def filter_ml(example):\n",
    "    return '#ml#>' in example['translations']\n",
    "\n",
    "# Apply the filter to the 'test' split\n",
    "test_ml_pairs = dataset_dict_shuffled['test'].filter(filter_ml)\n",
    "\n",
    "# Create a new DatasetDict for the filtered pairs\n",
    "dataset_dict_ml_test = DatasetDict({\n",
    "    \"test_ml\": test_ml_pairs\n",
    "})\n",
    "\n",
    "# Print the structure and size of the new DatasetDict\n",
    "print(dataset_dict_ml_test)\n",
    "\n",
    "# Accessing information about the new DatasetDict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict_ml_test['test_ml']['translations'][-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Log in to Hugging Face Hub\n",
    "api_token = 'Your token'\n",
    "login(api_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "model_name = \"ABHIiiii1/FineTuned-Trans-oneTomany-llama-2-7b\"\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load the model with the quantization configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "\n",
    "# Load the tokenizer for the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translator(text, language_code):\n",
    "    prompt = text + \" \" + language_code\n",
    "    tokenized_input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = tokenized_input[\"input_ids\"].cuda()\n",
    "\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        num_beams=6,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=130\n",
    "    )\n",
    "    for seq in generation_output.sequences:\n",
    "        output = tokenizer.decode(seq, skip_special_tokens=True)\n",
    "        return output.split(language_code)[1].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"hello, what is your name?\"\n",
    "language_code = \"#ml#>\"\n",
    "translated_text = translator(text, language_code)\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"hello, what is your name?\"\n",
    "language_code = \"#hi#>\"\n",
    "translated_text = translator(text, language_code)\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_texts(translator, dataset, language_code):\n",
    "    tgt_texts, trans_texts = [], []\n",
    "\n",
    "    for translation in dataset['translations']:\n",
    "        src_text, tgt_text = translation.split(language_code)\n",
    "        translated_text = translator(src_text.strip(), language_code)\n",
    "        tgt_texts.append(tgt_text.strip())\n",
    "        trans_texts.append(translated_text)\n",
    "\n",
    "    return tgt_texts, trans_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_texts, trans_texts = translate_texts(translator, dataset_dict_hi_test['test_hi'], \"#hi#>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_textsML, trans_textsML = translate_texts(translator, dataset_dict_ml_test['test_ml'], \"#ml#>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file name\n",
    "file_name = \"MT5_Bi_en_hi_pred.txt\"\n",
    "\n",
    "# Open the file in write mode and save the list\n",
    "with open(file_name, \"w\") as file:\n",
    "    for item in trans_texts2:\n",
    "        file.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: attrs in c:\\users\\asus\\anaconda3\\envs\\test_env\\lib\\site-packages (23.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade evaluate sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.144445924670746\n",
      "33.827890772364086\n",
      "74.7157615282507\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "ter = evaluate.load(\"ter\")\n",
    "\n",
    "with open(\"D:\\\\IASNLP2\\\\mT5\\\\Results\\\\MT5_Bi_hi_en_pred.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    pred = f.readlines()\n",
    "\n",
    "with open(\"D:\\\\IASNLP2\\\\mT5\\\\Results\\\\MT5_Bi_hi_en_tgt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ref = f.readlines()\n",
    "\n",
    "new_ref = []\n",
    "for sent in ref:\n",
    "    new_ref.append([sent])\n",
    "\n",
    "print(sacrebleu.compute(predictions=pred, references=new_ref)[\"score\"])\n",
    "print(chrf.compute(predictions=pred, references=new_ref)[\"score\"])\n",
    "print(ter.compute(predictions=pred, references=new_ref)[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.710726570086187\n",
      "31.063934840100497\n",
      "74.16267942583733\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "ter = evaluate.load(\"ter\")\n",
    "\n",
    "with open(\"D:\\IASNLP2\\mT5\\Results\\MT5_Bi_en_hi_pred.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    pred = f.readlines()\n",
    "\n",
    "with open(\"D:\\IASNLP2\\mT5\\Results\\MT5_Bi_en_hi_tgt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ref = f.readlines()\n",
    "\n",
    "new_ref = []\n",
    "for sent in ref:\n",
    "    new_ref.append([sent])\n",
    "\n",
    "print(sacrebleu.compute(predictions=pred, references=new_ref)[\"score\"])\n",
    "print(chrf.compute(predictions=pred, references=new_ref)[\"score\"])\n",
    "print(ter.compute(predictions=pred, references=new_ref)[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.480294983932118\n",
      "19.61848203050378\n",
      "84.78219444338579\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "ter = evaluate.load(\"ter\")\n",
    "\n",
    "with open(\"D:\\IASNLP2\\mT5\\Results\\en-hi-pred-mt5.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    pred = f.readlines()\n",
    "\n",
    "with open(\"D:\\IASNLP2\\mT5\\Results\\en-hi-tgt-mt5.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ref = f.readlines()\n",
    "\n",
    "new_ref = []\n",
    "for sent in ref:\n",
    "    new_ref.append([sent])\n",
    "\n",
    "print(sacrebleu.compute(predictions=pred, references=new_ref)[\"score\"])\n",
    "print(chrf.compute(predictions=pred, references=new_ref)[\"score\"])\n",
    "print(ter.compute(predictions=pred, references=new_ref)[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0885475872120058\n",
      "16.23827656125403\n",
      "91.93984371160367\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "ter = evaluate.load(\"ter\")\n",
    "\n",
    "with open(\"D:\\IASNLP2\\mT5\\Results\\en-bgl-pred-mt5.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    pred = f.readlines()\n",
    "\n",
    "with open(\"D:\\IASNLP2\\mT5\\Results\\en-bgl-tgt-mt5.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ref = f.readlines()\n",
    "\n",
    "new_ref = []\n",
    "for sent in ref:\n",
    "    new_ref.append([sent])\n",
    "\n",
    "print(sacrebleu.compute(predictions=pred, references=new_ref)[\"score\"])\n",
    "print(chrf.compute(predictions=pred, references=new_ref)[\"score\"])\n",
    "print(ter.compute(predictions=pred, references=new_ref)[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7545549128295486\n",
      "15.699038778529001\n",
      "92.93261905932079\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "ter = evaluate.load(\"ter\")\n",
    "\n",
    "with open(\"D:\\IASNLP2\\mT5\\Results\\hi-bgl-pred-mt5.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    pred = f.readlines()\n",
    "\n",
    "with open(\"D:\\IASNLP2\\mT5\\Results\\hi-bgl-tgt-mt5.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ref = f.readlines()\n",
    "\n",
    "new_ref = []\n",
    "for sent in ref:\n",
    "    new_ref.append([sent])\n",
    "\n",
    "print(sacrebleu.compute(predictions=pred, references=new_ref)[\"score\"])\n",
    "print(chrf.compute(predictions=pred, references=new_ref)[\"score\"])\n",
    "print(ter.compute(predictions=pred, references=new_ref)[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.223732432350877\n",
      "23.225896622542557\n",
      "84.66852067820938\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "ter = evaluate.load(\"ter\")\n",
    "\n",
    "with open(\"D:\\IASNLP2\\mT5\\Results\\hi-en-pred-mt5.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    pred = f.readlines()\n",
    "\n",
    "with open(\"D:\\IASNLP2\\mT5\\Results\\hi-en-tgt-mt5.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ref = f.readlines()\n",
    "\n",
    "new_ref = []\n",
    "for sent in ref:\n",
    "    new_ref.append([sent])\n",
    "\n",
    "print(sacrebleu.compute(predictions=pred, references=new_ref)[\"score\"])\n",
    "print(chrf.compute(predictions=pred, references=new_ref)[\"score\"])\n",
    "print(ter.compute(predictions=pred, references=new_ref)[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9469035214522026\n",
      "21.58559951457612\n",
      "86.72288508118775\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "ter = evaluate.load(\"ter\")\n",
    "\n",
    "with open(\"D:/IASNLP2/mT5/Results/bgl-en-pred-mt5.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    pred = f.readlines()\n",
    "\n",
    "with open(\"D:/IASNLP2/mT5/Results/bgl-en-tgt-mt5.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ref = f.readlines()\n",
    "\n",
    "new_ref = []\n",
    "for sent in ref:\n",
    "    new_ref.append([sent])\n",
    "\n",
    "print(sacrebleu.compute(predictions=pred, references=new_ref)[\"score\"])\n",
    "print(chrf.compute(predictions=pred, references=new_ref)[\"score\"])\n",
    "print(ter.compute(predictions=pred, references=new_ref)[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.145890057066503\n",
      "16.923500821126947\n",
      "88.20839208811311\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "ter = evaluate.load(\"ter\")\n",
    "\n",
    "with open(\"D:/IASNLP2/mT5/Results/bgl-hi-pred-mt5.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    pred = f.readlines()\n",
    "\n",
    "with open(\"D:/IASNLP2/mT5/Results/bgl-hi-tgt-mt5.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ref = f.readlines()\n",
    "\n",
    "new_ref = []\n",
    "for sent in ref:\n",
    "    new_ref.append([sent])\n",
    "\n",
    "print(sacrebleu.compute(predictions=pred, references=new_ref)[\"score\"])\n",
    "print(chrf.compute(predictions=pred, references=new_ref)[\"score\"])\n",
    "print(ter.compute(predictions=pred, references=new_ref)[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02659026043067876\n",
      "7.121786236387546\n",
      "94.09500609013398\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "ter = evaluate.load(\"ter\")\n",
    "\n",
    "with open(\"D:\\IASNLP2\\mT5\\Results\\llama_1_M_en_hi_pred.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    pred = f.readlines()\n",
    "\n",
    "with open(\"D:\\IASNLP2\\mT5\\Results\\llama_1_M_en_hi_tgt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ref = f.readlines()\n",
    "\n",
    "new_ref = []\n",
    "for sent in ref:\n",
    "    new_ref.append([sent])\n",
    "\n",
    "print(sacrebleu.compute(predictions=pred, references=new_ref)[\"score\"])\n",
    "print(chrf.compute(predictions=pred, references=new_ref)[\"score\"])\n",
    "print(ter.compute(predictions=pred, references=new_ref)[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IASNLP2\\mT5\\Results\\llamaenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04095977898080804\n",
      "6.853082539094396\n",
      "96.4312546957175\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "ter = evaluate.load(\"ter\")\n",
    "\n",
    "with open(\"D:\\IASNLP2\\mT5\\Results\\llama_one2many_en_ml_pred.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    pred = f.readlines()\n",
    "\n",
    "with open(\"D:\\IASNLP2\\mT5\\Results\\llama_one2many_en_ml_tgt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ref = f.readlines()\n",
    "\n",
    "new_ref = []\n",
    "for sent in ref:\n",
    "    new_ref.append([sent])\n",
    "\n",
    "print(sacrebleu.compute(predictions=pred, references=new_ref)[\"score\"])\n",
    "print(chrf.compute(predictions=pred, references=new_ref)[\"score\"])\n",
    "print(ter.compute(predictions=pred, references=new_ref)[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SacreBLEU: {'score': 14.144445924670746, 'counts': [6595, 3326, 1977, 1245], 'totals': [12614, 11614, 10614, 9614], 'precisions': [52.283177421912164, 28.63785086964009, 18.62634256642171, 12.949864780528396], 'bp': 0.5769818284614675, 'sys_len': 12614, 'ref_len': 19551}\n",
      "chrF: {'score': 33.827890772364086, 'char_order': 6, 'word_order': 0, 'beta': 2}\n",
      "TER: {'score': 74.7157615282507, 'num_edits': 12946, 'ref_length': 17327.0}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load evaluation metrics\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "ter = evaluate.load(\"ter\")\n",
    "\n",
    "# Read predictions and references with the correct encoding\n",
    "with open(\"D:\\\\IASNLP2\\\\mT5\\\\Results\\\\MT5_Bi_hi_en_pred.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    pred = f.readlines()\n",
    "\n",
    "with open(\"D:\\\\IASNLP2\\\\mT5\\\\Results\\\\MT5_Bi_hi_en_tgt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ref = f.readlines()\n",
    "\n",
    "# Prepare the reference list\n",
    "new_ref = []\n",
    "for sent in ref:\n",
    "    new_ref.append([sent.strip()])  # Strip newline characters\n",
    "\n",
    "# Calculate metrics\n",
    "sacrebleu_result = sacrebleu.compute(predictions=pred, references=new_ref)\n",
    "chrf_result = chrf.compute(predictions=pred, references=new_ref)\n",
    "ter_result = ter.compute(predictions=pred, references=new_ref)\n",
    "\n",
    "print(\"SacreBLEU:\", sacrebleu_result)\n",
    "print(\"chrF:\", chrf_result)\n",
    "print(\"TER:\", ter_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
