{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import  numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../datasets/wiki/\"\n",
    "import os\n",
    "datatypes = os.listdir(PATH)\n",
    "datatypes.remove(\".DS_Store\")\n",
    "\n",
    "def get_contents(file_path):\n",
    "    sentences = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            sentence = line.strip()\n",
    "            sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "def get_duals(datatypes,lang):\n",
    "    for i in datatypes:\n",
    "        x = os.listdir(os.path.join(PATH,i))\n",
    "        x.remove(\"domain.txt\")\n",
    "        if lang in x[0] or lang in x[1]:\n",
    "            res_lang = get_contents(os.path.join(PATH,i,x[0]))\n",
    "            res_en = get_contents(os.path.join(PATH,i,\"train.eng_Latn\"))\n",
    "            return {lang:res_lang,\"en\":res_en}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_duals(datatypes,\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hi</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"संयुक्त प्रतिस्पर्धा\" वा \"बहु प्रतिस्पर्धा \" ...</td>\n",
       "      <td>There are also \"combined events\" or \"multi eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>आवेदन जमा करबाक बाद, अहाँकेँ किछु व्यावसायिक द...</td>\n",
       "      <td>After submitting your application, you should ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>१९९० के दशक मे दमानिया शिपिंग द्वारा संचालित म...</td>\n",
       "      <td>There was also a short-lived catamaran service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>एहन रोगीक लेल, जनिक संक्रमण आ कैंसरक नैदानिक स...</td>\n",
       "      <td>X-rays are a less costly initial option offere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>एलर्जी सँ पीड़ित नेत्रश्लेष्माशोथक लेल, मुह के...</td>\n",
       "      <td>For allergic conjunctivitis, cool water poured...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>यहाँ के पौधों की कुछ प्रजातियों में यूपेटोरियम...</td>\n",
       "      <td>Some of the plant species include eupatorium, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>यह आदिलाबाद जिले के नेरेडीगोंडा के पास तरनाम ख...</td>\n",
       "      <td>It is approximately 5km from Tarnam Khurd, nea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>सवातुला गुण्डम जलप्रपात भारत के तेलंगाना राज्य...</td>\n",
       "      <td>Savatula Gundam Waterfalls is one of many wate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>केम्पटी जलप्रपात को 1835 में ब्रिटिश अधिकारी ज...</td>\n",
       "      <td>Kempty Falls were developed as a tourist desti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>जैसे, रक्षात्मक शैली से अपेक्षा की जाने वाली न...</td>\n",
       "      <td>As such, knockouts are more common than one wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    hi  \\\n",
       "0    \"संयुक्त प्रतिस्पर्धा\" वा \"बहु प्रतिस्पर्धा \" ...   \n",
       "1    आवेदन जमा करबाक बाद, अहाँकेँ किछु व्यावसायिक द...   \n",
       "2    १९९० के दशक मे दमानिया शिपिंग द्वारा संचालित म...   \n",
       "3    एहन रोगीक लेल, जनिक संक्रमण आ कैंसरक नैदानिक स...   \n",
       "4    एलर्जी सँ पीड़ित नेत्रश्लेष्माशोथक लेल, मुह के...   \n",
       "..                                                 ...   \n",
       "395  यहाँ के पौधों की कुछ प्रजातियों में यूपेटोरियम...   \n",
       "396  यह आदिलाबाद जिले के नेरेडीगोंडा के पास तरनाम ख...   \n",
       "397  सवातुला गुण्डम जलप्रपात भारत के तेलंगाना राज्य...   \n",
       "398  केम्पटी जलप्रपात को 1835 में ब्रिटिश अधिकारी ज...   \n",
       "399  जैसे, रक्षात्मक शैली से अपेक्षा की जाने वाली न...   \n",
       "\n",
       "                                                    en  \n",
       "0    There are also \"combined events\" or \"multi eve...  \n",
       "1    After submitting your application, you should ...  \n",
       "2    There was also a short-lived catamaran service...  \n",
       "3    X-rays are a less costly initial option offere...  \n",
       "4    For allergic conjunctivitis, cool water poured...  \n",
       "..                                                 ...  \n",
       "395  Some of the plant species include eupatorium, ...  \n",
       "396  It is approximately 5km from Tarnam Khurd, nea...  \n",
       "397  Savatula Gundam Waterfalls is one of many wate...  \n",
       "398  Kempty Falls were developed as a tourist desti...  \n",
       "399  As such, knockouts are more common than one wo...  \n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data)[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/var/folders/5n/k7vs285n7vz44p74rryzf67h0000gn/T/ipykernel_1373/2619750898.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Training loss: 2.599706707149744\n",
      "Epoch 1/3, Validation loss: 0.08676095977425576\n",
      "Epoch 2/3, Training loss: 0.11623244918882847\n",
      "Epoch 2/3, Validation loss: 0.025831568986177444\n",
      "Epoch 3/3, Training loss: 0.07209880966693163\n",
      "Epoch 3/3, Validation loss: 0.019819531589746475\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from transformers import XLNetTokenizer, XLNetLMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TextToTextDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "def preprocess_data(file_path, tokenizer):\n",
    "    # Load the data into a pandas DataFrame\n",
    "    df = pd.DataFrame(data)[:400]\n",
    "    \n",
    "    # Encode the data using the tokenizer\n",
    "    encodings = tokenizer(df['hi'].tolist(), df['en'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    return encodings\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, epochs, learning_rate):\n",
    "    # Set up the optimizer and learning rate scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    \n",
    "    # Move the model to the device (GPU/CPU)\n",
    "    device = torch.device(\"cpu\") \n",
    "    model.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch, labels=batch[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training loss: {avg_train_loss}\")\n",
    "\n",
    "        model.eval()\n",
    "        total_eval_loss = 0\n",
    "        for batch in val_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch, labels=batch[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_eval_loss / len(val_dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Validation loss: {avg_val_loss}\")\n",
    "\n",
    "def main():\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "    model = XLNetLMHeadModel.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "    # Load and preprocess the data\n",
    "    file_path = 'path_to_your_mt_data.tsv'\n",
    "    encodings = preprocess_data(file_path, tokenizer)\n",
    "    \n",
    "    # Create a PyTorch dataset\n",
    "    dataset = TextToTextDataset(encodings)\n",
    "\n",
    "    # Split the dataset into training and validation sets\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "    # Train the model\n",
    "    train(model, train_dataloader, val_dataloader, epochs=3, learning_rate=2e-5)\n",
    "    torch.save(model.state_dict(), \"XLnet.pt\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLNetLMHeadModel(\n",
       "  (transformer): XLNetModel(\n",
       "    (word_embedding): Embedding(32000, 768)\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation_function): GELUActivation()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_loss): Linear(in_features=768, out_features=32000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XLNetLMHeadModel.from_pretrained('xlnet-base-cased')\n",
    "model.load_state_dict(torch.load(\"XLnet.pt\"))\n",
    "model.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4036,    4,    3,   19,   19,   19,   19,   19,   19,   19,   19,   19,\n",
      "           19,   19,   19,   19,   19,   19,   19,   19,   19,   19,   19,   19,\n",
      "           19,   19,   19,   19,   19,   19,   19,   19,   19,   19,   19,   19,\n",
      "           19,   19,   19,   19,   19,   19,   19,   19,   19,   19,   19,   19,\n",
      "           19,   19]])\n",
      "Hi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import XLNetTokenizer, XLNetLMHeadModel\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "model = XLNetLMHeadModel.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "# Load the state dictionary from the file\n",
    "model.load_state_dict(torch.load(\"XLnet.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# Prepare the input data\n",
    "input_text = \"\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids, max_length=50, num_return_sequences=1, temperature=1.0)\n",
    "    print(outputs)\n",
    "\n",
    "# Decode the output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a boy\n",
      "I am happy.\n",
      "मैं जानता हूँ\n",
      "मला ओळखलं पाहिजे\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration, AutoModelForSeq2SeqLM\n",
    "from transformers import AlbertTokenizer, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/IndicBART\", do_lower_case=False, use_fast=False, keep_accents=True)\n",
    "\n",
    "# Or use tokenizer = AlbertTokenizer.from_pretrained(\"ai4bharat/IndicBART\", do_lower_case=False, use_fast=False, keep_accents=True)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"ai4bharat/IndicBART\")\n",
    "\n",
    "# Or use model = MBartForConditionalGeneration.from_pretrained(\"ai4bharat/IndicBART\")\n",
    "\n",
    "# Some initial mapping\n",
    "bos_id = tokenizer._convert_token_to_id_with_added_voc(\"<s>\")\n",
    "eos_id = tokenizer._convert_token_to_id_with_added_voc(\"</s>\")\n",
    "pad_id = tokenizer._convert_token_to_id_with_added_voc(\"<pad>\")\n",
    "# To get lang_id use any of ['<2as>', '<2bn>', '<2en>', '<2gu>', '<2hi>', '<2kn>', '<2ml>', '<2mr>', '<2or>', '<2pa>', '<2ta>', '<2te>']\n",
    "\n",
    "# First tokenize the input and outputs. The format below is how IndicBART was trained so the input should be \"Sentence </s> <2xx>\" where xx is the language code. Similarly, the output should be \"<2yy> Sentence </s>\". \n",
    "inp = tokenizer(\"I am a boy </s> <2en>\", add_special_tokens=False, return_tensors=\"pt\", padding=True).input_ids # tensor([[  466,  1981,    80, 25573, 64001, 64004]])\n",
    "\n",
    "out = tokenizer(\"<2hi> मैं  एक लड़का हूँ </s>\", add_special_tokens=False, return_tensors=\"pt\", padding=True).input_ids # tensor([[64006,   942,    43, 32720,  8384, 64001]])\n",
    "# Note that if you use any language other than Hindi or Marathi, you should convert its script to Devanagari using the Indic NLP Library.\n",
    "\n",
    "model_outputs=model(input_ids=inp, decoder_input_ids=out[:,0:-1], labels=out[:,1:])\n",
    "\n",
    "# For loss\n",
    "model_outputs.loss ## This is not label smoothed.\n",
    "\n",
    "# For logits\n",
    "model_outputs.logits\n",
    "\n",
    "# For generation. Pardon the messiness. Note the decoder_start_token_id.\n",
    "\n",
    "model.eval() # Set dropouts to zero\n",
    "\n",
    "model_output=model.generate(inp, use_cache=True, num_beams=4, max_length=20, min_length=1, early_stopping=True, pad_token_id=pad_id, bos_token_id=bos_id, eos_token_id=eos_id, decoder_start_token_id=tokenizer._convert_token_to_id_with_added_voc(\"<2en>\"))\n",
    "\n",
    "\n",
    "# Decode to get output strings\n",
    "\n",
    "decoded_output=tokenizer.decode(model_output[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "print(decoded_output) # I am a boy\n",
    "# Note that if your output language is not Hindi or Marathi, you should convert its script from Devanagari to the desired language using the Indic NLP Library.\n",
    "\n",
    "# What if we mask?\n",
    "\n",
    "inp = tokenizer(\"I am [MASK] </s> <2en>\", add_special_tokens=False, return_tensors=\"pt\", padding=True).input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-07-01 16:39:09.050579: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\n",
      "- configuration_falcon.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n",
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\n",
      "- modeling_falcon.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtiiuae/falcon-7b\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model)\n\u001b[0;32m----> 8\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m sequences \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m     17\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGirafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDaniel: Hello, Girafatron!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGirafatron:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     eos_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/pipelines/__init__.py:895\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    894\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 895\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    906\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/pipelines/base.py:283\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to load the model with Tensorflow.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 283\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    285\u001b[0m         model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:559\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/modeling_utils.py:3199\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3195\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3196\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeepSpeed Zero-3 is not compatible with `low_cpu_mem_usage=True` or with passing a `device_map`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3197\u001b[0m         )\n\u001b[1;32m   3198\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 3199\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   3200\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3201\u001b[0m         )\n\u001b[1;32m   3203\u001b[0m \u001b[38;5;66;03m# handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\u001b[39;00m\n\u001b[1;32m   3204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit \u001b[38;5;129;01mor\u001b[39;00m load_in_8bit:\n",
      "\u001b[0;31mImportError\u001b[0m: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"tiiuae/falcon-7b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "sequences = pipeline(\n",
    "   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
