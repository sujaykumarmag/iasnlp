{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-06-30 21:24:20.628606: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import XLNetTokenizer, XLNetLMHeadModel, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Custom Dataset\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, input_texts, target_texts, tokenizer, max_length=128):\n",
    "        self.input_texts = input_texts\n",
    "        self.target_texts = target_texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.input_texts[idx]\n",
    "        target_text = self.target_texts[idx]\n",
    "        inputs = self.tokenizer(input_text, add_special_tokens=True, max_length=self.max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "        targets = self.tokenizer(target_text, add_special_tokens=True, max_length=self.max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        target_ids = targets['input_ids'].squeeze()\n",
    "        return input_ids, target_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 5.417823314666748\n",
      "Epoch 1, Loss: 2.0797722339630127\n",
      "Epoch 2, Loss: 0.9125195145606995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (-1). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: I am a boyळम्ळम्ळम्ळम्ळम्ळम्ळम्ळम्ळम्ळम्ळम् कधीहीळम् कधीहीळम् कधीहीळम् कधीही कधीही कधीहीळम् कधीहीळम्ळम् कधीहीळम्ळम्ळम्ळम् कधीहीळम् कधीहीळम् कधीहीळम् कधीहीळम् कधीहीळम्ळम् कधीहीळम्\n"
     ]
    }
   ],
   "source": [
    "from mbart.configuration_mbart import MBartConfig\n",
    "from mbart.modeling_mbart import MBartModel, MBartForConditionalGeneration\n",
    "from mbart.tokenization_mbart import MBartTokenizer\n",
    "\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "from transformers import AlbertTokenizer, AutoTokenizer\n",
    "\n",
    "\n",
    "from xlnet.modeling_xlnet import XLNetLMHeadModel\n",
    "from xlnet.configuration_xlnet import XLNetConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/IndicBART\", do_lower_case=False, use_fast=False, keep_accents=True)\n",
    "\n",
    "\n",
    "\n",
    "dec_only_config = XLNetConfig(vocab_size = 64014, bos_token_id= 64000, n_layer=6,pad_token_id=0,eos_token_id=64001)\n",
    "model = XLNetLMHeadModel(config=dec_only_config)\n",
    "\n",
    "# Example data\n",
    "input_texts = [\"I am a boy </s> <2en>\"]\n",
    "target_texts = [\"<2hi> मैं एक लड़का हूँ </s>\"]\n",
    "\n",
    "# Prepare the dataset and dataloader\n",
    "dataset = TranslationDataset(input_texts, target_texts, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Define training parameters\n",
    "epochs = 3\n",
    "learning_rate = 2e-5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for input_ids, target_ids in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, labels=target_ids)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "torch.save(model.state_dict(), \"finetuned_xlnet.pt\")\n",
    "\n",
    "# Testing the model in inference time\n",
    "model.eval()\n",
    "test_input = \"I am a boy </s> <2en>\"\n",
    "test_input_ids = tokenizer(test_input, add_special_tokens=True, return_tensors=\"pt\").input_ids\n",
    "generated_ids = model.generate(test_input_ids, max_length=50, num_return_sequences=1)\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(f\"Generated text: {generated_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (-1). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a boy <2en> in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import XLNetTokenizer, XLNetLMHeadModel, MBartConfig, MBartForConditionalGeneration\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "dec_only_model = XLNetLMHeadModel.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "# Example input and target texts\n",
    "x = \"I am a boy </s> <2en>\"\n",
    "y = \"<2hi> मैं एक लड़का हूँ </s>\"\n",
    "\n",
    "# Tokenize the input and target texts\n",
    "inp = tokenizer(x, add_special_tokens=True, return_tensors=\"pt\", padding=True).input_ids\n",
    "out = tokenizer(y, add_special_tokens=True, return_tensors=\"pt\", padding=True).input_ids\n",
    "\n",
    "# Generate output using the model\n",
    "# output_dec = dec_only_model(input_ids=inp, labels=out)\n",
    "\n",
    "# Decode the generated output\n",
    "dec_only_out = tokenizer.decode(dec_only_model.generate(inp, max_length=50, num_return_sequences=1)[0], skip_special_tokens=True)\n",
    "\n",
    "print(dec_only_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
